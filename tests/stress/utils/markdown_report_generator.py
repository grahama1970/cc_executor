#!/usr/bin/env python3
"""
Markdown report generator for stress test results.

Converts test results into well-formatted markdown reports with:
- Summary statistics
- Category breakdowns
- Success/failure indicators
- Timing information
- Pattern matching results
"""

import json
from datetime import datetime
from typing import Dict, List, Any, Optional


class MarkdownReportGenerator:
    """Generate markdown reports from stress test results."""
    
    def __init__(self):
        self.timestamp = datetime.now()
        
    def generate_report(
        self, 
        results: Dict[str, Any], 
        title: str = "Stress Test Report",
        include_responses: bool = True
    ) -> str:
        """Generate a comprehensive markdown report."""
        
        report = []
        
        # Header
        report.append(f"# {title}")
        report.append("")
        report.append(f"**Generated:** {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Summary section
        report.extend(self._generate_summary(results))
        
        # Results by category
        if "categories" in results:
            report.extend(self._generate_category_results(results["categories"], include_responses))
        elif "detailed_results" in results:
            report.extend(self._generate_adaptive_results(results))
            
        # Footer
        report.append("")
        report.append("---")
        report.append(f"*Report generated by CC Executor Stress Test Suite*")
        
        return "\n".join(report)
    
    def _generate_summary(self, results: Dict[str, Any]) -> List[str]:
        """Generate summary statistics section."""
        
        lines = []
        lines.append("## Summary")
        lines.append("")
        
        if "summary" in results:
            # Adaptive test runner format
            summary = results["summary"]
            lines.append("| Metric | Value |")
            lines.append("|--------|-------|")
            lines.append(f"| Total Tests | {summary.get('total_tests', 0)} |")
            lines.append(f"| Successful | {summary.get('successful_tests', 0)} |")
            lines.append(f"| Success Rate | {summary.get('success_rate', 0)*100:.1f}% |")
            lines.append(f"| Total Attempts | {summary.get('total_attempts', 0)} |")
            lines.append(f"| Avg Attempts/Test | {summary.get('average_attempts', 0):.1f} |")
            
            if "strategy_usage" in summary and summary["strategy_usage"]:
                lines.append("")
                lines.append("### Retry Strategy Usage")
                lines.append("")
                lines.append("| Strategy | Count |")
                lines.append("|----------|-------|")
                for strategy, count in summary["strategy_usage"].items():
                    lines.append(f"| {strategy} | {count} |")
                    
        else:
            # Standard format
            total_tests = results.get("total_tests", 0)
            successful = results.get("successful_tests", 0)
            failed = results.get("failed_tests", 0)
            
            lines.append("| Metric | Value |")
            lines.append("|--------|-------|")
            lines.append(f"| Total Tests | {total_tests} |")
            lines.append(f"| ✅ Successful | {successful} |")
            lines.append(f"| ❌ Failed | {failed} |")
            lines.append(f"| Success Rate | {(successful/total_tests*100) if total_tests > 0 else 0:.1f}% |")
            lines.append(f"| Total Duration | {results.get('total_duration', 0):.2f}s |")
        
        lines.append("")
        return lines
    
    def _generate_category_results(self, categories: Dict[str, Any], include_responses: bool) -> List[str]:
        """Generate results by category."""
        
        lines = []
        lines.append("## Results by Category")
        lines.append("")
        
        for category, tasks in categories.items():
            lines.append(f"### {category.title()}")
            lines.append("")
            
            # Category summary
            total = len(tasks)
            successful = sum(1 for t in tasks if t.get("success", False))
            
            lines.append(f"**Total:** {total} | **Success:** {successful} | **Failed:** {total - successful}")
            lines.append("")
            
            # Task details
            for task in tasks:
                lines.extend(self._generate_task_result(task, include_responses))
                lines.append("")
                
        return lines
    
    def _generate_adaptive_results(self, results: Dict[str, Any]) -> List[str]:
        """Generate results for adaptive test format."""
        
        lines = []
        lines.append("## Detailed Results")
        lines.append("")
        
        detailed_results = results.get("detailed_results", [])
        
        # Group by final success
        successful = [r for r in detailed_results if r["final_success"]]
        failed = [r for r in detailed_results if not r["final_success"]]
        
        if successful:
            lines.append("### ✅ Successful Tests")
            lines.append("")
            for result in successful:
                lines.extend(self._generate_adaptive_task_result(result))
                lines.append("")
                
        if failed:
            lines.append("### ❌ Failed Tests")
            lines.append("")
            for result in failed:
                lines.extend(self._generate_adaptive_task_result(result))
                lines.append("")
                
        return lines
    
    def _generate_task_result(self, task: Dict[str, Any], include_response: bool) -> List[str]:
        """Generate markdown for a single task result."""
        
        lines = []
        
        # Task header
        status = "✅" if task.get("success", False) else "❌"
        lines.append(f"#### {status} {task.get('task_name', 'Unknown')} (`{task.get('task_id', 'unknown')}`)")
        lines.append("")
        
        # Metadata
        lines.append(f"- **Duration:** {task.get('duration', 0):.2f}s")
        if task.get("error"):
            lines.append(f"- **Error:** {task['error']}")
            
        # Request
        request = task.get("request", "")
        if request:
            lines.append("")
            lines.append("**Request:**")
            lines.append("```")
            lines.append(request[:200] + ("..." if len(request) > 200 else ""))
            lines.append("```")
            
        # Pattern verification
        patterns = task.get("pattern_matches", {})
        if patterns:
            lines.append("")
            lines.append("**Pattern Verification:**")
            for pattern, found in patterns.items():
                status = "✓" if found else "✗"
                lines.append(f"- {status} `{pattern}`")
                
        # Response preview
        if include_response and task.get("full_response"):
            lines.append("")
            lines.append("**Response Preview:**")
            lines.append("```")
            response = task["full_response"][:500]
            lines.append(response + ("..." if len(task["full_response"]) > 500 else ""))
            lines.append("```")
            
        return lines
    
    def _generate_adaptive_task_result(self, result: Dict[str, Any]) -> List[str]:
        """Generate markdown for adaptive test result."""
        
        lines = []
        
        # Header
        status = "✅" if result["final_success"] else "❌"
        lines.append(f"#### {status} {result['test_name']} (`{result['test_id']}`)")
        lines.append("")
        
        # Summary
        lines.append(f"- **Total Attempts:** {result['total_attempts']}")
        lines.append(f"- **Total Duration:** {result['total_duration']:.1f}s")
        if result.get("retry_strategies_used"):
            lines.append(f"- **Retry Strategies:** {', '.join(result['retry_strategies_used'])}")
            
        # Original request
        lines.append("")
        lines.append("**Original Request:**")
        lines.append("```")
        lines.append(result['original_request'][:200] + ("..." if len(result['original_request']) > 200 else ""))
        lines.append("```")
        
        # Attempts
        if len(result["attempts"]) > 1:
            lines.append("")
            lines.append("**Attempts:**")
            lines.append("")
            
            for i, attempt in enumerate(result["attempts"]):
                lines.append(f"{i+1}. Duration: {attempt['duration']:.1f}s | "
                           f"Success: {'✅' if attempt['success'] else '❌'}")
                if attempt.get("error_type"):
                    lines.append(f"   - Error: `{attempt['error_type']}` - {attempt.get('error_message', 'N/A')}")
                if i > 0:  # Show modified request for retries
                    lines.append(f"   - Modified: `{attempt['request'][:100]}...`")
                    
        return lines


def convert_txt_to_markdown(txt_file_path: str, output_path: Optional[str] = None) -> str:
    """Convert an existing text report to markdown format."""
    
    with open(txt_file_path, 'r') as f:
        content = f.read()
        
    # Basic conversion rules
    lines = content.split('\n')
    markdown_lines = []
    
    for line in lines:
        # Convert headers
        if line.startswith("=" * 80):
            continue
        elif line.startswith("UNIFIED STRESS TEST"):
            markdown_lines.append("# Unified Stress Test Report")
        elif line.startswith("CATEGORY:"):
            markdown_lines.append(f"## {line}")
        elif line.startswith("Task:"):
            markdown_lines.append(f"### {line}")
        elif line.startswith("─" * 60):
            markdown_lines.append("---")
        # Convert success/failure indicators
        elif "✅ Yes" in line:
            markdown_lines.append(line.replace("Success:", "**Status:**"))
        elif "❌ No" in line:
            markdown_lines.append(line.replace("Success:", "**Status:**"))
        # Format pattern results
        elif line.strip().startswith("✓") or line.strip().startswith("✗"):
            markdown_lines.append(f"- {line.strip()}")
        else:
            markdown_lines.append(line)
            
    markdown_content = "\n".join(markdown_lines)
    
    if output_path:
        with open(output_path, 'w') as f:
            f.write(markdown_content)
            
    return markdown_content


if __name__ == "__main__":
    # Example usage
    generator = MarkdownReportGenerator()
    
    # Example results
    example_results = {
        "summary": {
            "total_tests": 10,
            "successful_tests": 8,
            "success_rate": 0.8,
            "total_attempts": 15,
            "average_attempts": 1.5,
            "strategy_usage": {
                "token_limit": 3,
                "rate_limit": 2
            }
        },
        "detailed_results": [
            {
                "test_id": "test_1",
                "test_name": "Simple Test",
                "original_request": "What is 2+2?",
                "final_success": True,
                "total_attempts": 1,
                "total_duration": 5.2,
                "retry_strategies_used": [],
                "attempts": [
                    {"duration": 5.2, "success": True}
                ]
            }
        ]
    }
    
    report = generator.generate_report(example_results, "Example Stress Test Report")
    print(report)