# Core Assessment Report Template

This template defines the structure for core module assessment reports that include both automated testing results and Claude's manual reasonableness assessments.

## Report Metadata
```yaml
Report Type: Core Module Assessment
Generated By: Automated Script + Claude Analysis
Template Version: 1.0
Purpose: Verify all core components have functioning usage examples
```

## Required Sections

### 1. Report Header
```markdown
# Core Components Usage Assessment Report
Generated: {timestamp}
Session ID: {session_id}
Assessed by: Claude (Script: assess_all_core_usage.py + Manual Analysis)
Template: docs/templates/CORE_ASSESSMENT_REPORT_TEMPLATE.md v1.0
```

### 2. Executive Summary
```markdown
## Summary
- Total Components Tested: {count}
- Automated Pass Rate: {auto_pass_rate}%
- Claude's Verified Pass Rate: {claude_pass_rate}%
- Critical Component (websocket_handler.py): {status}
- System Health: {overall_assessment}
```

### 3. Component Assessment Format

Each component MUST include both automated results AND Claude's analysis:

```markdown
### {icon} {filename}

#### Automated Test Results
- **Exit Code**: {code}
- **Execution Time**: {time}s  
- **Output Lines**: {lines}
- **Expected Indicators Found**: {indicators}
- **Contains Numbers**: {yes/no}

#### üß† Claude's Reasonableness Assessment
**Verdict**: [REASONABLE/UNREASONABLE/SUSPICIOUS]

**Expected vs Actual**:
- **Expected**: {what the component should demonstrate}
- **Observed**: {what the output actually shows}

**Evidence Analysis**:
‚úì {specific evidence point that proves functionality}
‚úì {another specific evidence point}
‚úó {anything missing or concerning}

**Numerical Validation**:
- {analysis of any numbers in output - are they sensible?}
- {e.g., "PID 806282 is valid Linux process ID"}
- {e.g., "CPU usage 5.0% is realistic for idle system"}

**Hallucination Check**:
- Claims without evidence: {list any unverifiable claims}
- Output inconsistencies: {list any contradictions}
- Suspicious patterns: {list any red flags}

**Conclusion**: {1-2 sentences on whether this proves the component works}

#### Complete JSON Response File
```json
{entire_json_file_content_with_line_numbers}
```
**CRITICAL: Show the ENTIRE JSON file with line numbers from the Read tool - this proves you actually read it!**

#### Key Output (from "output" field)
```
{extracted_output_for_readability}
```

Showing the full JSON prevents hallucination - it's harder to fake the entire file structure with timestamps, metadata, etc.

#### Verification Commands (if applicable)
```bash
# Commands to independently verify this component works
{verification_commands}
```

**Output Truncation Notes** (if applicable):
- Large outputs (>10KB) may be truncated in assessment scripts
- Binary content shows as: `[BINARY DATA - {size} bytes total, preview: {hex}]`
- Always read the full JSON file for complete output
- Never trust assessment script summaries - read the source
```

### 4. Overall System Assessment

```markdown
## üéØ Claude's Overall System Assessment

### System Health Analysis
Based on the outputs, I assess the cc_executor core system as: [HEALTHY/DEGRADED/FAILING]

**Key Observations**:
1. {Major finding from outputs}
2. {Pattern noticed across components}
3. {Any concerning indicators}

### Confidence in Results
**Confidence Level**: [HIGH/MEDIUM/LOW]

**Reasoning**: {Why I have this confidence level based on the evidence}

### Risk Assessment
- **Immediate Risks**: {Any critical issues found}
- **Potential Issues**: {Things that might become problems}
- **Unknown Factors**: {What couldn't be verified}
```

### 5. Recommendations

```markdown
## üìã Recommendations

### Immediate Actions
1. {Critical fixes needed based on assessment}

### Improvements
1. {Suggested enhancements based on output analysis}

### Future Monitoring
1. {What to watch for in future assessments}
```

## Assessment Guidelines for Claude

### What Makes Output "REASONABLE"?
1. **Functional Demonstration**: Output shows the component doing its intended job
2. **Valid Data**: Numbers are in expected ranges, strings match expected formats
3. **No Unexpected Errors**: No stack traces unless testing error handling
4. **Logical Flow**: Output sequence makes sense for the operation
5. **Performance**: Execution times are appropriate for the task

### Red Flags to Check
- üö® Infinite loops or recursion indicators
- üö® Memory/resource warnings
- üö® Permission or access errors
- üö® Missing expected functionality
- üö® Suspicious timing (too fast/slow)

### Number Validation Checklist
- [ ] Process IDs: Valid range (1-32768 typical)
- [ ] Ports: Valid range (1-65535)
- [ ] Percentages: 0-100%
- [ ] Timeouts: Reasonable for operation type
- [ ] Memory sizes: Power of 2 typical
- [ ] Exit codes: 0=success, non-zero=error

## Example Component Assessment

### ‚úÖ resource_monitor.py

#### Automated Test Results
- **Exit Code**: 0
- **Execution Time**: 3.37s
- **Output Lines**: 17
- **Expected Indicators Found**: cpu, resource, usage, %
- **Contains Numbers**: Yes

#### üß† Claude's Reasonableness Assessment
**Verdict**: REASONABLE

**Expected vs Actual**:
- **Expected**: CPU/GPU monitoring with dynamic timeout adjustment based on load
- **Observed**: Clear demonstration of resource monitoring and threshold-based timeout multiplication

**Evidence Analysis**:
‚úì CPU readings (5.0%, 2.8%) are realistic for a development machine
‚úì Timeout multiplier correctly applies 3x when CPU > 14%
‚úì Boundary testing at exactly 14% shows proper threshold handling
‚úì GPU at 0% is expected without active GPU tasks

**Numerical Validation**:
- CPU percentages all within valid 0-100% range
- 3.37s execution time reasonable for sampling system resources
- Timeout calculations correct: 30s ‚Üí 90s with 3x multiplier

**Conclusion**: Output proves the resource monitor correctly reads system metrics and adjusts timeouts based on load, which is essential for preventing premature timeout of long-running tasks.

## Template Usage Notes

1. **Every component needs Claude's assessment** - Not just pass/fail
2. **Be specific** - Reference actual values from output
3. **Explain reasoning** - Why does this output prove functionality?
4. **Check the numbers** - Validate all numerical outputs
5. **Consider the context** - What is this component's role in the system?
6. **READ THE JSON FILES** - Do NOT use truncated output from the assessment script
   - JSON files are in: `/src/cc_executor/core/tmp/responses/`
   - Each file contains the COMPLETE output in the "output" field
   - NEVER include "[truncated]" text in the report

## Anti-Hallucination Measures

Following best practices from TASK_LIST_REPORT_TEMPLATE.md:

1. **Include full JSON output** - Always show entire JSON file with line numbers from Read tool
2. **Show verification commands** - Include commands to verify component functionality
3. **Evidence-based assessment** - Every claim must reference specific output lines
4. **Cross-reference outputs** - Check consistency across related components
5. **Timestamp validation** - Ensure timestamps are sequential and reasonable
6. **Session tracking** - Include session IDs to trace back to actual execution
7. **UUID4 Verification** - Every assessment execution generates a unique UUID4 that appears:
   - At the END of JSON result files (hardest to fake)
   - In the markdown report's Anti-Hallucination Verification section
   - In separate verification JSON files
   - This UUID proves the assessment actually ran and wasn't hallucinated

### Verification Command Examples
```bash
# Verify component is executable
ls -la /path/to/component.py
stat /path/to/component.py

# Check imports are valid
python -c "import cc_executor.core.component"

# Verify output files exist
ls -la /src/cc_executor/core/tmp/responses/

# Check process actually ran
ps aux | grep component_name
```

## Cross-Component Validation

When assessing multiple components:

### Dependency Verification
- Components that import each other should both pass
- Shared utilities (e.g., truncate_logs.py) should work across all users
- Configuration changes should be reflected in dependent components

### Pattern Consistency
- Similar components should have similar execution times
- Related components should show compatible output formats
- Error patterns should be consistent across the codebase

## Assessment Report Creation

After running all assessment scripts, create comprehensive report:

```bash
# 1. Run all assessments
python src/cc_executor/core/prompts/scripts/assess_all_core_usage.py
python src/cc_executor/hooks/prompts/scripts/assess_all_hooks_usage.py
python src/cc_executor/cli/prompts/scripts/assess_all_cli_usage.py
python src/cc_executor/client/prompts/scripts/assess_all_client_usage.py

# 2. Create comprehensive report following this template
# Include:
# - Summary statistics across all directories
# - Detailed analysis for each component
# - Cross-validation of related components
# - Overall system health assessment
```

## UUID4 Anti-Hallucination Verification

### How It Works

1. **Generation**: Each assessment script execution generates a fresh UUID4
2. **Placement**: The UUID4 is placed at the END of JSON objects (hardest position to fake)
3. **Verification**: The same UUID4 appears in:
   - The markdown report's Anti-Hallucination Verification section
   - The JSON results file with key `execution_uuid`
   - Individual component response JSON files
   - Can be searched in transcript logs

### Example Verification

```bash
# After running assessment, verify UUID matches
REPORT_UUID=$(grep "Report UUID" CORE_USAGE_REPORT_*.md | cut -d'`' -f2)
JSON_UUID=$(jq -r '.execution_uuid' CORE_USAGE_RESULTS_*.json)

if [ "$REPORT_UUID" = "$JSON_UUID" ]; then
    echo "‚úÖ UUID verification passed - report is genuine"
else
    echo "‚ùå UUID mismatch - possible hallucination"
fi
```

### In Reports

Every assessment report MUST include:

```markdown
## Anti-Hallucination Verification
**Report UUID**: `{uuid4_here}`

This UUID4 is generated fresh for this report execution and can be verified against:
- JSON response files saved during execution
- Transcript logs for this session

If this UUID does not appear in the corresponding JSON files, the report may be hallucinated.
```

## Version History
- v1.0 (2025-07-03): Initial template addressing v8 failure - requires Claude's reasonableness assessments
- v1.1 (2025-07-03): Updated to address v9 failure - MUST include complete outputs from JSON files, no truncation allowed
- v1.2 (2025-07-04): Added anti-hallucination measures and cross-validation from TASK_LIST_REPORT_TEMPLATE.md
- v1.3 (2025-07-04): Added UUID4 anti-hallucination verification system