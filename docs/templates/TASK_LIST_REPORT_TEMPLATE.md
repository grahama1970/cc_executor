# Task List Report Template

This template defines the structure for task list execution reports that include both automated results AND agent reasonableness assessments for each task.

## Report Metadata
```yaml
Report Type: Task List Execution Report
Generated By: Task List Completion Hook + Agent Analysis
Template Version: 1.0
Purpose: Comprehensive verification of task list execution with anti-hallucination measures
```

## Required Sections

### 1. Report Header
```markdown
# Task List Execution Report
Generated: {timestamp}
Session ID: {session_id}
Task List: {task_list_path}
Assessed by: {agent_name} (Hook: task_list_completion_report.py + Manual Analysis)
Template: docs/templates/TASK_LIST_REPORT_TEMPLATE.md v1.0
```

### 2. Executive Summary
```markdown
## Summary
- Total Tasks Executed: {count}
- Automated Success Rate: {auto_success_rate}%
- Agent Verified Success Rate: {agent_success_rate}%
- System Health: {overall_health}
- Confidence Level: {HIGH/MEDIUM/LOW}
```

### 3. Pre-Flight Assessment
```markdown
## Pre-Flight Check Results
- Predicted Success Rate: {predicted_rate}%
- Actual Success Rate: {actual_rate}%
- Prediction Accuracy: {accuracy}
- Risk Assessment Was: {risk_level}
```

### 4. Task Assessment Format

Each task MUST include ALL of the following sections in this exact order:

```markdown
### {icon} Task {number}: {description}

#### Automated Execution Results
- **Status**: {status}
- **Exit Code**: {code}
- **Duration**: {time}s  
- **Output Lines**: {lines}
- **Files Created**: {count}
- **Files Modified**: {count}

#### 🧠 Agent's Reasonableness Assessment
**Verdict**: [REASONABLE/UNREASONABLE/SUSPICIOUS/HALLUCINATED]

**Task Intent Analysis**:
- **What was requested**: {specific task requirement}
- **What was delivered**: {actual outcome}
- **Match percentage**: {0-100}%

**Evidence Verification**:
✓ {specific evidence that proves task completion}
✓ {file path verification - did claimed files get created?}
✗ {missing expected outcomes}

**Output Analysis**:
- **Contains expected keywords**: {yes/no} - {list keywords}
- **Output length appropriate**: {yes/no} - {explanation}
- **Error indicators present**: {yes/no} - {details if yes}

**File System Verification**:
```bash
# Commands to verify claimed file operations
ls -la {claimed_file_path}
cat {claimed_file_path} | head -20
```

**Hallucination Check**:
- Claims without evidence: {list any}
- Unverifiable statements: {list any}
- Suspicious patterns: {list any}

**Conclusion**: {1-2 sentences on whether this task actually succeeded}

#### Complete Raw JSON Output
```json
{
  "task_number": {number},
  "description": "{full_task_description}",
  "status": "{status}",
  "exit_code": {code},
  "duration": {seconds},
  "output_lines": {count},
  "files_created": [{list}],
  "files_modified": [{list}],
  "evidence": [{list}],
  "warnings": [{list}],
  "error_message": "{error_if_any}",
  "raw_output": "{complete_stdout_stderr_output}",
  "timestamp": "{iso_timestamp}",
  "session_id": "{session_id}",
  "execution_id": "{unique_id}",
  "output_truncated": {boolean},
  "execution_uuid": "{uuid4}"  // UUID4 at END for anti-hallucination
}
```

**Output Truncation Notes**:
- Large outputs (>10KB) are truncated inline to prevent report bloat
- Binary content shows as: `[BINARY DATA - {size} bytes total, preview: {hex}]`
- Full output saved to: `reports/full_outputs/task_{number}_full_output.txt`
- `output_truncated: true` indicates truncation occurred

#### Key Output Extract (Human Readable)
```
{first 50 lines of raw_output for quick reference}
```
```

### 5. Cross-Task Validation
```markdown
## 🔗 Cross-Task Dependencies Verification

### Sequential Dependency Check
- Task 2 required output from Task 1: {PASSED/FAILED}
  - Expected: {what Task 1 should have provided}
  - Found: {what was actually available}
- Task 3 required output from Task 2: {PASSED/FAILED}
  - Expected: {what Task 2 should have provided}
  - Found: {what was actually available}

### File Continuity Check
- Files created in earlier tasks still exist: {YES/NO}
- File modifications are cumulative: {YES/NO}
- No file conflicts detected: {YES/NO}
```

### 6. Overall System Assessment
```markdown
## 🎯 Agent's Overall Assessment

### Execution Quality Analysis
Based on individual task assessments, the overall execution quality is: [EXCELLENT/GOOD/FAIR/POOR]

**Statistical Breakdown**:
- Tasks marked REASONABLE: {count} ({percentage}%)
- Tasks marked SUSPICIOUS: {count} ({percentage}%)
- Tasks marked HALLUCINATED: {count} ({percentage}%)

### Confidence in Results
**Overall Confidence**: [HIGH/MEDIUM/LOW]

**Confidence Factors**:
- ✓ All claimed files verified to exist
- ✓ Output patterns consistent with success
- ✗ Some outputs could not be independently verified
- ? Unclear whether all side effects completed

### Pattern Analysis
**Positive Patterns Observed**:
1. {pattern noticed across successful tasks}
2. {common success indicators}

**Concerning Patterns**:
1. {pattern noticed in failed tasks}
2. {common failure indicators}
```

### 7. Recommendations
```markdown
## 📋 Recommendations

### For Failed Tasks
{For each failed task, specific fix recommendation}

### For Suspicious Tasks  
{For each suspicious task, verification steps}

### For Future Executions
1. {Improvement based on patterns observed}
2. {Timeout adjustments needed}
3. {Task decomposition suggestions}
```

### 8. Appendices
```markdown
## 📎 Appendices

### A. Environment Snapshot
```json
{
  "python_version": "{version}",
  "working_directory": "{path}",
  "virtual_env": "{path}",
  "installed_packages": [{list}],
  "environment_variables": {
    "PYTHONPATH": "{value}",
    "ANTHROPIC_API_KEY": "{masked}"
  }
}
```

### B. Redis Metrics
```json
{
  "task_metrics": {
    "average_duration_by_complexity": {},
    "failure_rate_by_complexity": {},
    "timeout_adjustments_made": {}
  }
}
```

### C. Verification Commands Log
```bash
# All commands run to verify task outputs
{command_history}
```
```

## Assessment Guidelines

### What Makes a Task "REASONABLE"?

1. **Output Matches Intent**
   - The task description clearly matches what was produced
   - No significant gaps between request and delivery

2. **Verifiable Claims**
   - Every "Created file X" has a corresponding file
   - Every "Test passed" has test output
   - Every "Function works" has execution proof

3. **Appropriate Output Volume**
   - Simple tasks have concise output
   - Complex tasks have detailed output
   - Output length matches task complexity

4. **No Hallucination Indicators**
   - No claims without evidence
   - No "I did X" without showing X
   - No success claims with error messages

### Red Flags for "SUSPICIOUS" or "HALLUCINATED"

- 🚨 "Successfully created" with no file path
- 🚨 "Tests pass" with no test output  
- 🚨 "Function works" with no execution
- 🚨 Very short output for complex task
- 🚨 Success claimed but exit code ≠ 0
- 🚨 Files claimed but `ls` shows nothing

### Verification Checklist

For EVERY task, verify:
- [ ] Exit code matches claimed status
- [ ] Output length appropriate for task
- [ ] All file claims can be verified
- [ ] Test results shown if tests mentioned
- [ ] Error messages align with failures
- [ ] Timestamps are sequential and reasonable
- [ ] Dependencies from previous tasks available

## Example Task Assessment

### ✅ Task 1: Create FastAPI TODO Application

#### Automated Execution Results
- **Status**: success
- **Exit Code**: 0
- **Duration**: 45.3s  
- **Output Lines**: 127
- **Files Created**: 2
- **Files Modified**: 0

#### 🧠 Agent's Reasonableness Assessment
**Verdict**: REASONABLE

**Task Intent Analysis**:
- **What was requested**: Create folder todo_api/ with main.py containing GET /todos, POST /todos, and DELETE /todos/{id} endpoints
- **What was delivered**: Exactly these files and endpoints created
- **Match percentage**: 100%

**Evidence Verification**:
✓ Created todo_api/main.py (verified via ls output in raw JSON)
✓ File contains all three requested endpoints (seen in cat output)
✓ Used in-memory storage as requested (todos = [] visible)

**Output Analysis**:
- **Contains expected keywords**: YES - "Creating", "todo_api", "main.py", "GET", "POST", "DELETE"
- **Output length appropriate**: YES - 127 lines reasonable for file creation with content display
- **Error indicators present**: NO

**File System Verification**:
```bash
ls -la todo_api/
# Output: -rw-rw-r-- 1.0k user 4 Jul 08:57 main.py ✓

cat todo_api/main.py | grep -E "(GET|POST|DELETE)"
# Output shows all three endpoints ✓
```

**Hallucination Check**:
- Claims without evidence: None
- Unverifiable statements: None  
- Suspicious patterns: None

**Conclusion**: Task completed successfully with all requirements met and independently verified.

#### Complete Raw JSON Output
```json
{
  "task_number": 1,
  "description": "Create FastAPI TODO application with GET /todos, POST /todos, and DELETE /todos/{id} endpoints using in-memory storage",
  "status": "success",
  "exit_code": 0,
  "duration": 45.3,
  "output_lines": 127,
  "files_created": ["todo_api/main.py"],
  "files_modified": [],
  "evidence": [
    "Created file at: todo_api/main.py",
    "Added GET /todos endpoint",
    "Added POST /todos endpoint", 
    "Added DELETE /todos/{id} endpoint"
  ],
  "warnings": [],
  "error_message": null,
  "raw_output": "Creating todo_api directory...\nWriting main.py with FastAPI application...\n\nFile created at: todo_api/main.py\nContents:\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\napp = FastAPI()\n\n# In-memory storage\ntodos: List[dict] = []\ntodo_counter = 0\n\n\nclass TodoCreate(BaseModel):\n    title: str\n    completed: bool = False\n\n\nclass Todo(BaseModel):\n    id: int\n    title: str\n    completed: bool\n\n\n@app.get(\"/todos\", response_model=List[Todo])\nasync def get_todos():\n    return todos\n\n\n@app.post(\"/todos\", response_model=Todo)\nasync def create_todo(todo: TodoCreate):\n    global todo_counter\n    todo_counter += 1\n    new_todo = {\n        \"id\": todo_counter,\n        \"title\": todo.title,\n        \"completed\": todo.completed\n    }\n    todos.append(new_todo)\n    return new_todo\n\n\n@app.delete(\"/todos/{todo_id}\")\nasync def delete_todo(todo_id: int):\n    for i, todo in enumerate(todos):\n        if todo[\"id\"] == todo_id:\n            todos.pop(i)\n            return {\"message\": \"Todo deleted successfully\"}\n    raise HTTPException(status_code=404, detail=\"Todo not found\")\n```\n\nVerifying file creation...\n$ ls -la todo_api/\ntotal 4\ndrwxrwxr-x 2 user user 4096 Jul  4 08:57 .\ndrwxrwxr-x 5 user user 4096 Jul  4 08:57 ..\n-rw-rw-r-- 1 user user 1026 Jul  4 08:57 main.py\n\nAPI structure:\n- GET /todos - Returns all todos\n- POST /todos - Creates a new todo\n- DELETE /todos/{id} - Deletes a todo by ID\n\nIn-memory storage initialized as empty list.\nReady to run with: uvicorn todo_api.main:app --reload",
  "timestamp": "2025-07-04T08:57:45.123456",
  "session_id": "ws_session_abc123",
  "execution_id": "exec_001_abc"
}
```

## Usage Notes

1. **Raw JSON is mandatory** - Never truncate or summarize
2. **Agent assessment required** - Not just pass/fail
3. **Verification commands** - Show actual filesystem checks
4. **Cross-task validation** - Ensure sequential dependencies work
5. **Confidence levels** - Be honest about uncertainty
6. **Evidence-based** - Every claim must have proof

## Anti-Hallucination Measures

1. **Include full raw output** - Makes it impossible to fake
2. **Show verification commands** - Prove files exist
3. **Cross-reference task outputs** - Check consistency
4. **Timestamp everything** - Detect temporal impossibilities
5. **Session IDs** - Trace back to actual execution
6. **UUID4 Verification** - Every execution generates a unique UUID4 that appears:
   - At the END of JSON result files (hardest to fake)
   - In the report's Anti-Hallucination Verification section
   - Can be verified against transcript logs
   - Proves the assessment actually ran

## UUID4 Anti-Hallucination Verification

### Implementation in Reports

Every task list report MUST include:

```markdown
## Anti-Hallucination Verification
**Report UUID**: `{uuid4_here}`

This UUID4 is generated fresh for this report execution and can be verified against:
- JSON response files saved during execution
- Individual task execution JSON files
- Transcript logs for this session

If this UUID does not appear in the corresponding JSON files, the report may be hallucinated.
```

### Verification Process

1. **Check report UUID against JSON**:
   ```bash
   REPORT_UUID=$(grep "Report UUID" TASK_LIST_REPORT_*.md | cut -d'`' -f2)
   JSON_UUID=$(jq -r '.execution_uuid' TASK_LIST_RESULTS_*.json)
   
   if [ "$REPORT_UUID" = "$JSON_UUID" ]; then
       echo "✅ UUID verification passed"
   else
       echo "❌ UUID mismatch - possible hallucination"
   fi
   ```

2. **Verify in transcript**:
   ```bash
   rg "$REPORT_UUID" ~/.claude/projects/-*/*.jsonl
   ```

3. **Cross-check task UUIDs**:
   ```bash
   # Each task should have its own UUID in addition to report UUID
   jq -r '.tasks[].execution_uuid' TASK_LIST_RESULTS_*.json | sort -u
   ```

## Version History
- v1.0 (2025-01-04): Initial template addressing need for rigorous task verification with raw JSON and agent assessments
- v1.1 (2025-07-04): Added UUID4 anti-hallucination verification system