# FINAL Issues Report for Gemini - Logger Agent Implementation

**Date**: 2025-01-14  
**Generated by**: Claude Code Assistant  
**Purpose**: Complete list of ALL issues that need fixing

## CRITICAL: Mock Implementations Are Forbidden

**IMPORTANT**: Claude discovered that several modules are mock implementations that don't use the database. This violates the no-mocking rule.

## Status Summary

### ✅ Actually Working (1 script)
- `src/utils/log_utils.py` - Real utility functions, no database needed

### ⚠️ Partially Fixed by Claude (1 script)
- `src/arango_init.py` - Fixed imports and asyncio issues, now connects to DB

### ❌ BROKEN - Mock Implementations (3 scripts)
**These are NOT working - they're mocks that must be replaced:**
- `src/arangodb/core/search/hybrid_search.py` - Returns fake search results
- `src/arangodb/core/graph/relationship_extraction.py` - Returns fake relationships
- `src/arangodb/core/memory/memory_agent.py` - Uses local dict instead of DB

### ❌ Need Asyncio Fixes (2 scripts)
- `src/arango_log_sink.py` - Import errors, needs python-arango conversion
- `src/agent_log_manager.py` - Import errors, needs python-arango conversion

## What Claude Fixed

1. **Environment Setup**:
```bash
cd /home/graham/workspace/experiments/cc_executor/proof_of_concept/logger_agent
uv init --no-workspace
uv add python-arango aiofiles psutil tenacity numpy uvloop python-dotenv loguru setuptools
uv add "urllib3<2"  # Critical for compatibility
```

2. **Fixed arango_init.py** - Example of the pattern to apply:
```python
# Changed from:
from aioarango import ArangoClient

# To:
from arango import ArangoClient

# Wrapped sync operations:
def _create_database_sync():
    client = ArangoClient(hosts=os.getenv("ARANGO_URL"))
    # ... all sync operations ...

async def create_database_and_collections():
    return await asyncio.to_thread(_create_database_sync)
```

## Critical Issues Gemini Must Fix

### 1. Replace ALL Mock Implementations

**hybrid_search.py** - Must implement REAL database searches:
```python
class HybridSearch:
    def __init__(self, db):
        self.db = db  # MUST USE THIS!
        
    async def search(self, query: str, search_type: str = "bm25", ...):
        # MUST execute real AQL queries like:
        aql = """
        FOR doc IN log_events_view
        SEARCH ANALYZER(doc.message IN TOKENS(@query, 'text_en'), 'text_en')
        SORT BM25(doc) DESC
        LIMIT @limit
        RETURN doc
        """
        # Use asyncio.to_thread with self.db.aql.execute()
```

**relationship_extraction.py** - Must store in graph database:
```python
class RelationshipExtractor:
    def __init__(self, db):
        self.db = db  # MUST USE THIS!
        
    async def extract_relationships(self, log1: str, log2: str):
        # Extract relationships AND store them in log_causality edge collection
        # Use self.db.graph("log_relationships")
```

**memory_agent.py** - Must store in agent_learnings collection:
```python
class MemoryAgent:
    def __init__(self, db):
        self.db = db  # MUST USE THIS!
        
    async def add_memory(self, content: str, ...):
        # Store in self.db.collection("agent_learnings")
        # NOT in a local dictionary!
```

### 2. Fix Import Errors in Remaining Scripts

Apply the same pattern Claude used for arango_init.py to:
- `src/arango_log_sink.py`
- `src/agent_log_manager.py`

### 3. Update Deprecated Methods

Replace deprecated index creation methods:
- `add_persistent_index()` → `add_index({'type': 'persistent', ...})`
- `add_fulltext_index()` → `add_index({'type': 'fulltext', ...})`

## Full Code of Scripts Needing Fixes

### src/arangodb/core/search/hybrid_search.py (BROKEN - MOCK)
```python
#!/usr/bin/env python3
"""
Mock hybrid search module for Logger Agent integration.

This is a simplified mock implementation that provides the interface
expected by the Logger Agent without requiring the full ArangoDB
search infrastructure.
"""

# ... [FULL CODE - SEE FILE] ...

class HybridSearch:
    """Mock hybrid search implementation for testing."""
    
    def __init__(self, db=None):
        """Initialize with optional database connection."""
        self.db = db  # THIS IS IGNORED! MUST BE FIXED!
        logger.info("HybridSearch mock initialized")
    
    async def search(self, ...):
        # RETURNS FAKE DATA! MUST USE REAL DATABASE!
        mock_results = []
        if "error" in query.lower():
            mock_results.extend([{
                "_id": f"{collection}/123",
                "message": "Connection timeout error occurred",
                # ... FAKE DATA ...
            }])
        return mock_results  # THIS IS WRONG!
```

### src/arangodb/core/graph/relationship_extraction.py (BROKEN - MOCK)
```python
# Similar issue - returns fake relationships instead of using database
```

### src/arangodb/core/memory/memory_agent.py (BROKEN - MOCK)
```python
# Similar issue - uses local dict instead of database collection
```

### src/arango_log_sink.py (NEEDS ASYNCIO FIX)
```python
from aioarango import ArangoClient  # CHANGE TO: from arango import ArangoClient
# ... needs asyncio.to_thread wrapping for all DB operations
```

### src/agent_log_manager.py (NEEDS ASYNCIO FIX)
```python
from aioarango import ArangoClient  # CHANGE TO: from arango import ArangoClient
# ... needs asyncio.to_thread wrapping for all DB operations
```

## Testing Requirements

After fixes, ALL modules must:
1. Actually connect to and use the database
2. Fail if database is not available
3. Store/retrieve real data, not mocks
4. Use asyncio.to_thread for all DB operations

## Example Test
```python
# This MUST fail without database:
search = HybridSearch(None)
await search.search("test")  # Should raise exception!

# This MUST return real data from database:
search = HybridSearch(db)
results = await search.search("error")  # Real results, not mocks!
```

---

**CRITICAL**: The logger agent is currently non-functional because core modules are mocks. No data is actually being stored or searched in the database!

## Appendix: Full Script Contents

Below are the complete contents of all scripts referenced in this report:

### 1. `src/utils/log_utils.py` - ✅ WORKING

```python
#!/usr/bin/env python3
"""
Utility functions and helpers for log truncation and safe logging.

Provides functions to truncate large values (strings, lists, dicts) to make them
suitable for logging without overwhelming log storage. Handles special cases like
base64 image data URIs and large embeddings arrays.

Third-party Documentation:
- Python Logging: https://docs.python.org/3/library/logging.html
- Regular Expressions: https://docs.python.org/3/library/re.html

Example Input:
    {
        "id": 1,
        "description": "A very long description that exceeds 100 characters...",
        "embedding": [0.1, 0.2, 0.3, ... 1000 more elements],
        "image": "data:image/png;base64,iVBORw0KGgoAAAANS... (very long)"
    }

Expected Output:
    {
        "id": 1,
        "description": "A very long...characters...",
        "embedding": "[<1003 float elements>]",
        "image": "data:image/png;base64,iVBORw0KGg...AAASUVORK5CYII="
    }
"""

import asyncio
import json
import logging
import re
import sys
from pathlib import Path
from typing import List, Any, Dict, Optional
from datetime import datetime

from loguru import logger

# Configure logging
logger.remove()
logger.add(
    sys.stderr,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)

# Optional file logging
log_dir = Path(__file__).parent / "logs"
log_dir.mkdir(exist_ok=True)
logger.add(
    log_dir / f"{Path(__file__).stem}_{{time}}.log",
    rotation="10 MB",
    retention=5,
    level="DEBUG"
)

# Regex to identify common data URI patterns for images
BASE64_IMAGE_PATTERN = re.compile(r"^(data:image/[a-zA-Z+.-]+;base64,)")


# ============================================
# CORE FUNCTIONS (Outside __main__ block)
# ============================================

def truncate_large_value(
    value: Any,
    max_str_len: int = 100,
    max_list_elements_shown: int = 10,
) -> Any:
    """
    Truncate large strings or arrays to make them log-friendly.

    Handles base64 image strings by preserving the header and truncating the data.
    Summarizes lists/arrays longer than `max_list_elements_shown`.

    Args:
        value: The value to potentially truncate
        max_str_len: Maximum length for the data part of strings before truncation
        max_list_elements_shown: Maximum number of elements to show in arrays
                                 before summarizing the array instead.

    Returns:
        Truncated or original value
    """
    if isinstance(value, str):
        # Check if it's a base64 image data URI
        match = BASE64_IMAGE_PATTERN.match(value)
        if match:
            header = match.group(1)
            data = value[len(header) :]
            if len(data) > max_str_len:
                half_len = max_str_len // 2
                if half_len == 0 and max_str_len > 0:
                    half_len = 1
                truncated_data = (
                    f"{data[:half_len]}...{data[-half_len:]}" if half_len > 0 else "..."
                )
                return header + truncated_data
            else:
                return value
        # It's not a base64 image string, apply generic string truncation
        elif len(value) > max_str_len:
            half_len = max_str_len // 2
            if half_len == 0 and max_str_len > 0:
                half_len = 1
            return (
                f"{value[:half_len]}...{value[-half_len:]}" if half_len > 0 else "..."
            )
        else:
            return value

    elif isinstance(value, list):
        # Handle large lists (like embeddings) by summarizing
        if len(value) > max_list_elements_shown:
            if value:
                element_type = type(value[0]).__name__
                return f"[<{len(value)} {element_type} elements>]"
            else:
                return "[<0 elements>]"
        else:
            # If list elements are dicts, truncate them recursively
            return [
                truncate_large_value(item, max_str_len, max_list_elements_shown) 
                if isinstance(item, dict) else item 
                for item in value
            ]
    elif isinstance(value, dict):
        # Recursively truncate values within dictionaries
        return {
            k: truncate_large_value(v, max_str_len, max_list_elements_shown) 
            for k, v in value.items()
        }
    else:
        # Handle other types (int, float, bool, None, etc.) - return as is
        return value


def log_safe_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Create a log-safe version of the results list by truncating large fields
    within each dictionary.

    Args:
        results: List of documents (dictionaries) that may contain large fields.

    Returns:
        Log-safe version of the input list where large fields are truncated.

    Raises:
        TypeError: If the input `results` is not a list, or if any element
                   within the list is not a dictionary.
    """
    # Input Validation
    if not isinstance(results, list):
        raise TypeError(
            f"Expected input to be a List[Dict[str, Any]], but got {type(results).__name__}."
        )

    for index, item in enumerate(results):
        if not isinstance(item, dict):
            raise TypeError(
                f"Expected all elements in the input list to be dictionaries (dict), "
                f"but found element of type {type(item).__name__} at index {index}."
            )

    log_safe_output = []
    for doc in results:
        doc_copy = {}
        for key, value in doc.items():
            doc_copy[key] = truncate_large_value(value)
        log_safe_output.append(doc_copy)
    return log_safe_output


def log_api_request(service_name: str, request_data: Dict[str, Any], truncate: bool = True) -> None:
    """
    Log API request details.

    Args:
        service_name: Name of the service being called
        request_data: Request data to log
        truncate: Whether to truncate large values
    """
    if truncate:
        request_data_to_log = truncate_large_value(request_data)
    else:
        request_data_to_log = request_data

    logger.debug(f"{service_name} API Request: {request_data_to_log}")


def log_api_response(service_name: str, response_data: Any, truncate: bool = True) -> None:
    """
    Log API response details.

    Args:
        service_name: Name of the service being called
        response_data: Response data to log
        truncate: Whether to truncate large values
    """
    if truncate:
        response_data_to_log = truncate_large_value(response_data)
    else:
        response_data_to_log = response_data

    logger.debug(f"{service_name} API Response: {response_data_to_log}")


def log_api_error(service_name: str, error: Exception, request_data: Optional[Dict[str, Any]] = None) -> None:
    """
    Log API error details.

    Args:
        service_name: Name of the service being called
        error: The error that occurred
        request_data: Optional request data for context
    """
    error_message = f"{service_name} API Error: {str(error)}"

    if request_data:
        truncated_data = truncate_large_value(request_data)
        error_message += f" (Request: {truncated_data})"

    logger.error(error_message)


def save_results(results: Dict[str, Any], output_dir: Optional[Path] = None) -> Path:
    """
    Save results to a prettified JSON file.
    
    Args:
        results: Results dictionary to save
        output_dir: Optional output directory (defaults to tmp/responses)
        
    Returns:
        Path to saved file
    """
    if output_dir is None:
        output_dir = Path(__file__).parent / "tmp" / "responses"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{Path(__file__).stem}_results_{timestamp}.json"
    output_path = output_dir / filename
    
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, sort_keys=True)
    
    logger.info(f"Results saved to: {output_path}")
    return output_path


# ============================================
# USAGE EXAMPLES (Inside __main__ block)
# ============================================

async def working_usage():
    """
    Demonstrate log utility functions with stable, working examples.
    
    This function contains tested code that demonstrates the proper
    usage of truncation and log-safe functions.
    """
    logger.info("=== Running Working Usage Examples ===")
    
    # Example 1: Test basic truncation
    test_data = [
        {
            "id": 1,
            "description": "A short description.",
            "embedding": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
            "image_small": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=",
            "tags": ["short", "list"],
        },
        {
            "id": 2,
            "description": "This description is quite long, much longer than the default one hundred characters allowed, so it should definitely be truncated according to the rules specified in the function." * 2,
            "embedding": [float(i) / 100 for i in range(150)],
            "image_large": "data:image/jpeg;base64," + ("B" * 500),
            "tags": ["tag" + str(i) for i in range(20)],
        }
    ]
    
    # Test log_safe_results
    logger.info("Testing log_safe_results...")
    safe_results = log_safe_results(test_data)
    
    # Verify truncation worked
    assert len(safe_results) == 2
    assert "..." in safe_results[1]["description"]
    assert "[<150 float elements>]" in str(safe_results[1]["embedding"])
    logger.success("✓ log_safe_results working correctly")
    
    # Example 2: Test API logging functions
    logger.info("\nTesting API logging functions...")
    log_api_request("TestService", {"model": "test-model", "prompt": "A" * 200})
    log_api_response("TestService", {"result": "B" * 300, "status": "success"})
    log_api_error("TestService", Exception("Test error"), {"model": "test-model"})
    logger.success("✓ API logging functions working correctly")
    
    # Example 3: Test edge cases
    logger.info("\nTesting edge cases...")
    edge_cases = [
        {"empty_list": [], "none_value": None, "bool": True},
        {"nested": {"deep": {"deeper": {"value": "X" * 200}}}},
        {"mixed": [1, "string", {"nested": "value"}, None, True]}
    ]
    
    safe_edge_cases = log_safe_results(edge_cases)
    logger.success("✓ Edge cases handled correctly")
    
    # Save results
    save_results({
        "test_data": safe_results,
        "edge_cases": safe_edge_cases,
        "timestamp": datetime.utcnow().isoformat()
    })
    
    return True


async def debug_function():
    """
    Debug function for testing new features or investigating issues.
    
    Update this function freely when debugging truncation logic or
    testing specific edge cases.
    """
    logger.info("=== Running Debug Function ===")
    
    # Current debugging: Test very large data structures
    huge_data = {
        "massive_string": "X" * 10000,
        "huge_list": list(range(10000)),
        "deep_nesting": {}
    }
    
    # Build deep nesting
    current = huge_data["deep_nesting"]
    for i in range(100):
        current["level_" + str(i)] = {}
        current = current["level_" + str(i)]
    current["value"] = "Deep value"
    
    logger.info("Testing truncation on huge data structures...")
    truncated = truncate_large_value(huge_data)
    
    # Verify truncation
    assert len(str(truncated["massive_string"])) < 200
    assert "[<10000 int elements>]" in str(truncated["huge_list"])
    logger.success("✓ Large data structures truncated successfully")
    
    # Test invalid inputs
    logger.info("\nTesting error handling...")
    try:
        log_safe_results("not a list")  # Should raise TypeError
    except TypeError as e:
        logger.success(f"✓ Correctly caught error: {e}")
    
    try:
        log_safe_results([1, 2, 3])  # List of non-dicts
    except TypeError as e:
        logger.success(f"✓ Correctly caught error: {e}")
    
    return True


async def stress_test():
    """
    Run stress tests loaded from JSON configuration files.
    
    Tests high-volume truncation scenarios and performance.
    """
    logger.info("=== Running Stress Tests ===")
    
    # Generate stress test data
    stress_configs = [
        {
            "name": "high_volume",
            "description": "Test with many documents",
            "doc_count": 1000,
            "field_size": "large"
        },
        {
            "name": "deep_nesting",
            "description": "Test with deeply nested structures",
            "doc_count": 100,
            "nesting_depth": 50
        }
    ]
    
    for config in stress_configs:
        logger.info(f"\nRunning stress test: {config['name']}")
        
        if config["name"] == "high_volume":
            # Generate many documents
            docs = []
            for i in range(config["doc_count"]):
                doc = {
                    "id": i,
                    "data": "X" * 1000,
                    "embedding": list(range(500)),
                    "metadata": {"index": i, "test": True}
                }
                docs.append(doc)
            
            start_time = datetime.utcnow()
            safe_docs = log_safe_results(docs)
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            logger.success(
                f"✓ Processed {len(docs)} documents in {duration:.2f} seconds"
            )
            
        elif config["name"] == "deep_nesting":
            # Generate deeply nested documents
            docs = []
            for i in range(config["doc_count"]):
                doc = {"id": i}
                current = doc
                for level in range(config["nesting_depth"]):
                    current[f"level_{level}"] = {}
                    current = current[f"level_{level}"]
                current["value"] = f"Deep value {i}"
                docs.append(doc)
            
            start_time = datetime.utcnow()
            safe_docs = log_safe_results(docs)
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            logger.success(
                f"✓ Processed {len(docs)} nested documents in {duration:.2f} seconds"
            )
    
    logger.info("\n📊 Stress Test Summary: All tests passed")
    return True


if __name__ == "__main__":
    """
    Script entry point with triple-mode execution.
    
    Usage:
        python log_utils.py           # Runs working_usage() - stable tests
        python log_utils.py debug     # Runs debug_function() - experimental
        python log_utils.py stress    # Runs stress_test() - performance tests
    """
    import sys
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        """Main async entry point."""
        if mode == "debug":
            logger.info("Running in DEBUG mode...")
            success = await debug_function()
        elif mode == "stress":
            logger.info("Running in STRESS TEST mode...")
            success = await stress_test()
        else:
            logger.info("Running in WORKING mode...")
            success = await working_usage()
        
        return success
    
    # Single asyncio.run() call
    success = asyncio.run(main())
    exit(0 if success else 1)
```

### 2. `src/arango_init.py` - ⚠️ PARTIALLY FIXED

```python
#!/usr/bin/env python3
"""
arango_init.py - Initialize ArangoDB schema for Logger Agent

Creates database, collections, indexes, and ArangoSearch views.
Ensures idempotent execution for repeated runs.
"""

import asyncio
import os
import sys
from typing import Dict, Any, Optional
from pathlib import Path
from datetime import datetime

from arango import ArangoClient
from loguru import logger
from dotenv import load_dotenv
import uvloop

# Set uvloop as the event loop policy
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

# Load environment variables
load_dotenv()

# Configure logging
# For standalone scripts, removing default handler and adding a file/stderr handler is common.
# In a larger application, loguru configuration should be centralized.
logger.remove()
logger.add(
    sys.stderr,
    level="INFO"
)


def _create_database_sync():
    """Synchronous database creation logic."""
    # Connect to ArangoDB
    client = ArangoClient(hosts=os.getenv("ARANGO_URL", "http://localhost:8529"))
    
    # System database connection
    sys_db = client.db(
        "_system",
        username=os.getenv("ARANGO_USERNAME", "root"),
        password=os.getenv("ARANGO_PASSWORD", "openSesame")
    )
    
    db_name = os.getenv("ARANGO_DATABASE", "script_logs")
    
    # Create database if not exists
    if not sys_db.has_database(db_name):
        sys_db.create_database(db_name)
        logger.info(f"Created database: {db_name}")
    
    # Connect to our database
    db = client.db(
        db_name,
        username=os.getenv("ARANGO_USERNAME", "root"),
        password=os.getenv("ARANGO_PASSWORD", "openSesame")
    )
    
    # Create collections
    collections = {
        "log_events": {
            "schema": {
                "rule": {
                    "properties": {
                        "timestamp": {"type": "string"},
                        "level": {"type": "string"},
                        "message": {"type": "string"},
                        "execution_id": {"type": "string"},
                        "script_name": {"type": "string"},
                        "function_name": {"type": "string"},
                        "file_path": {"type": "string"},
                        "line_number": {"type": "integer"},
                        "extra_data": {"type": "object"},
                        "embeddings": {"type": "array"},
                        "tags": {"type": "array"}
                    },
                    "required": ["timestamp", "level", "message", "execution_id"]
                }
            }
        },
        "script_runs": {
            "schema": {
                "rule": {
                    "properties": {
                        "execution_id": {"type": "string"},
                        "script_name": {"type": "string"},
                        "start_time": {"type": "string"},
                        "end_time": {"type": "string"},
                        "status": {"type": "string"},
                        "metadata": {"type": "object"},
                        "error": {"type": "string"}
                    },
                    "required": ["execution_id", "script_name", "start_time"]
                }
            }
        },
        "agent_learnings": {
            "schema": {
                "rule": {
                    "properties": {
                        "timestamp": {"type": "string"},
                        "execution_id": {"type": "string"},
                        "learning": {"type": "string"},
                        "context": {"type": "object"},
                        "function_name": {"type": "string"},
                        "confidence": {"type": "number"}
                    },
                    "required": ["timestamp", "learning"]
                }
            }
        }
    }
    
    for coll_name, config in collections.items():
        if not db.has_collection(coll_name):
            collection = db.create_collection(
                coll_name,
                schema=config.get("schema")
            )
            logger.info(f"Created collection: {coll_name}")
        else:
            collection = db.collection(coll_name)
        
        # Create indexes
        if coll_name == "log_events":
            # Compound index for time-based queries
            collection.add_persistent_index(
                fields=["execution_id", "timestamp"],
                unique=False,
                sparse=False
            )
            
            # Index for level-based filtering
            collection.add_persistent_index(
                fields=["level", "timestamp"],
                unique=False,
                sparse=False
            )
            
            # Full-text index for message search
            collection.add_fulltext_index(
                fields=["message"],
                min_length=3
            )
            
        elif coll_name == "script_runs":
            # Unique index on execution_id
            collection.add_persistent_index(
                fields=["execution_id"],
                unique=True,
                sparse=False
            )
            
            # Index for script name queries
            collection.add_persistent_index(
                fields=["script_name", "start_time"],
                unique=False,
                sparse=False
            )
    
    # Create ArangoSearch view
    view_name = "log_events_view"
    existing_views = [v['name'] for v in db.views()]
    if view_name not in existing_views:
        db.create_arangosearch_view(
            view_name,
            properties={
                "links": {
                    "log_events": {
                        "analyzers": ["text_en", "identity"],
                        "fields": {
                            "message": {
                                "analyzers": ["text_en"]
                            },
                            "level": {
                                "analyzers": ["identity"]
                            },
                            "script_name": {
                                "analyzers": ["identity"]
                            },
                            "tags": {
                                "analyzers": ["identity"]
                            }
                        },
                        "includeAllFields": False,
                        "storeValues": "id",
                        "trackListPositions": False
                    }
                }
            }
        )
        logger.info(f"Created ArangoSearch view: {view_name}")
    
    # Create graph for log relationships (optional)
    graph_name = "log_relationships"
    existing_graphs = [g['name'] for g in db.graphs()]
    if graph_name not in existing_graphs:
        db.create_graph(
            graph_name,
            edge_definitions=[
                {
                    "edge_collection": "log_causality",
                    "from_vertex_collections": ["log_events"],
                    "to_vertex_collections": ["log_events", "agent_learnings"]
                }
            ]
        )
        logger.info(f"Created graph: {graph_name}")
    
    return db


async def create_database_and_collections():
    """Create database and collections with proper indexes (async wrapper)."""
    return await asyncio.to_thread(_create_database_sync)


async def working_usage():
    """Initialize database schema - stable working example."""
    logger.info("=== Initializing ArangoDB Schema ===")
    
    try:
        db = await create_database_and_collections()
        
        # Verify collections exist
        collections = await asyncio.to_thread(lambda: [c['name'] for c in db.collections()])
        logger.info(f"Available collections: {collections}")
        
        # Test write
        test_doc = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": "INFO",
            "message": "Database initialization test",
            "execution_id": "init_test_001",
            "script_name": "arango_init.py"
        }
        
        result = await asyncio.to_thread(db.collection("log_events").insert, test_doc)
        logger.success(f"Test document inserted: {result}")
        
        return True
        
    except Exception as e:
        logger.error(f"Database initialization failed: {e}")
        logger.exception("Full traceback:")
        return False


async def debug_function():
    """Debug function for testing schema modifications."""
    logger.info("=== Running Debug Mode ===")
    
    # Test experimental features
    def test_experimental():
        client = ArangoClient(hosts=os.getenv("ARANGO_URL", "http://localhost:8529"))
        db = client.db(
            os.getenv("ARANGO_DATABASE", "script_logs"),
            username=os.getenv("ARANGO_USERNAME", "root"),
            password=os.getenv("ARANGO_PASSWORD", "openSesame")
        )
        
        # Test APPROX_NEAR_COSINE availability
        test_vector = [0.1, 0.2, 0.3, 0.4, 0.5]
        query = """
        RETURN APPROX_NEAR_COSINE(@vector, @vector, 1)
        """
        
        try:
            cursor = db.aql.execute(query, bind_vars={"vector": test_vector})
            result = list(cursor)[0]
            logger.success(f"APPROX_NEAR_COSINE test passed: {result}")
        except Exception as e:
            logger.error(f"APPROX_NEAR_COSINE not available: {e}")
            logger.warning("Ensure --query.enable-experimental flag is set")
    
    await asyncio.to_thread(test_experimental)
    
    return True


if __name__ == "__main__":
    import sys
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        if mode == "debug":
            logger.info("Running in DEBUG mode...")
            success = await debug_function()
        else:
            logger.info("Running in WORKING mode...")
            success = await working_usage()
        
        return success
    
    success = asyncio.run(main())
    exit(0 if success else 1)
```

### 3. `src/arangodb/core/search/hybrid_search.py` - ❌ BROKEN - MOCK IMPLEMENTATION

```python
#!/usr/bin/env python3
"""
Mock hybrid search module for Logger Agent integration.

This is a simplified mock implementation that provides the interface
expected by the Logger Agent without requiring the full ArangoDB
search infrastructure.

Third-party Documentation:
- ArangoDB Search: https://www.arangodb.com/docs/stable/arangosearch.html

Example Input:
    query = "error connection timeout"
    search_type = "bm25"
    collection = "log_events"

Expected Output:
    [
        {"_id": "log_events/123", "message": "Connection timeout error", "score": 0.95},
        {"_id": "log_events/456", "message": "Database connection failed", "score": 0.87}
    ]
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

from loguru import logger

# Configure logging
logger.remove()
logger.add(
    sys.stderr,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)


# ============================================
# CORE FUNCTIONS (Outside __main__ block)
# ============================================

class HybridSearch:
    """Mock hybrid search implementation for testing."""
    
    def __init__(self, db=None):
        """Initialize with optional database connection."""
        self.db = db
        logger.info("HybridSearch mock initialized")
    
    async def search(
        self,
        query: str,
        search_type: str = "bm25",
        collection: str = "log_events",
        limit: int = 50,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Mock search implementation that returns realistic test data.
        
        Args:
            query: Search query string
            search_type: Type of search (bm25, semantic, hybrid)
            collection: Collection to search in
            limit: Maximum results to return
            filters: Optional filters to apply
            
        Returns:
            List of mock search results with scores
        """
        logger.info(f"Mock search: query='{query}', type={search_type}, collection={collection}")
        
        # Generate mock results based on query
        mock_results = []
        
        if "error" in query.lower():
            mock_results.extend([
                {
                    "_id": f"{collection}/123",
                    "_key": "123",
                    "message": "Connection timeout error occurred",
                    "level": "ERROR",
                    "timestamp": "2024-01-10T10:30:00",
                    "score": 0.95
                },
                {
                    "_id": f"{collection}/124",
                    "_key": "124", 
                    "message": "Database connection error",
                    "level": "ERROR",
                    "timestamp": "2024-01-10T10:31:00",
                    "score": 0.87
                }
            ])
        
        if "warning" in query.lower():
            mock_results.append({
                "_id": f"{collection}/125",
                "_key": "125",
                "message": "Memory usage warning: 85% utilized",
                "level": "WARNING",
                "timestamp": "2024-01-10T10:32:00",
                "score": 0.82
            })
        
        # Apply filters if provided
        if filters:
            if "level" in filters:
                mock_results = [r for r in mock_results if r.get("level") == filters["level"]]
        
        # Limit results
        mock_results = mock_results[:limit]
        
        logger.info(f"Returning {len(mock_results)} mock results")
        return mock_results


def save_results(results: Dict[str, Any], output_dir: Optional[Path] = None) -> Path:
    """Save results to JSON file."""
    if output_dir is None:
        output_dir = Path(__file__).parent / "tmp" / "responses"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{Path(__file__).stem}_results_{timestamp}.json"
    output_path = output_dir / filename
    
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, sort_keys=True)
    
    logger.info(f"Results saved to: {output_path}")
    return output_path


# ============================================
# USAGE EXAMPLES (Inside __main__ block)
# ============================================

async def working_usage():
    """
    Demonstrate HybridSearch mock functionality.
    
    This shows how the Logger Agent will use the search module.
    """
    logger.info("=== Running Working Usage Examples ===")
    
    # Initialize search
    search = HybridSearch()
    
    # Test 1: Search for errors
    logger.info("\nTest 1: Searching for errors...")
    error_results = await search.search(
        query="error connection",
        search_type="bm25",
        limit=10
    )
    
    logger.info(f"Found {len(error_results)} error results")
    for result in error_results:
        logger.info(f"  - {result['message'][:50]}... (score: {result['score']})")
    
    assert len(error_results) > 0, "Should find error results"
    
    # Test 2: Search with filters
    logger.info("\nTest 2: Searching with filters...")
    filtered_results = await search.search(
        query="error",
        filters={"level": "ERROR"},
        limit=5
    )
    
    logger.info(f"Found {len(filtered_results)} filtered results")
    assert all(r["level"] == "ERROR" for r in filtered_results), "All results should be ERROR level"
    
    # Test 3: Different search types
    logger.info("\nTest 3: Testing different search types...")
    for search_type in ["bm25", "semantic", "hybrid"]:
        results = await search.search(
            query="warning memory",
            search_type=search_type,
            limit=3
        )
        logger.info(f"  {search_type}: {len(results)} results")
    
    # Save test results
    save_results({
        "error_search": error_results,
        "filtered_search": filtered_results,
        "test_timestamp": datetime.utcnow().isoformat()
    })
    
    logger.success("✓ All search tests passed")
    return True


async def debug_function():
    """
    Debug function for testing search edge cases.
    
    Use this to test specific search scenarios or debug issues.
    """
    logger.info("=== Running Debug Function ===")
    
    search = HybridSearch()
    
    # Test empty query
    logger.info("Testing empty query...")
    empty_results = await search.search(query="", limit=5)
    logger.info(f"Empty query returned {len(empty_results)} results")
    
    # Test very long query
    logger.info("\nTesting very long query...")
    long_query = " ".join(["error"] * 100)
    long_results = await search.search(query=long_query, limit=5)
    logger.info(f"Long query returned {len(long_results)} results")
    
    # Test special characters
    logger.info("\nTesting special characters...")
    special_query = "error @#$%^&*()"
    special_results = await search.search(query=special_query, limit=5)
    logger.info(f"Special char query returned {len(special_results)} results")
    
    # Test collection variations
    logger.info("\nTesting different collections...")
    for collection in ["log_events", "agent_learnings", "script_runs"]:
        results = await search.search(
            query="test",
            collection=collection,
            limit=2
        )
        logger.info(f"  {collection}: {len(results)} results")
    
    return True


async def stress_test():
    """
    Stress test the search functionality.
    
    Tests high-volume and concurrent search operations.
    """
    logger.info("=== Running Stress Tests ===")
    
    search = HybridSearch()
    
    # Test 1: Many sequential searches
    logger.info("Test 1: Sequential search performance...")
    start_time = datetime.utcnow()
    
    for i in range(100):
        await search.search(f"query_{i}", limit=10)
        if i % 20 == 0:
            logger.info(f"  Completed {i} searches")
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(f"✓ 100 sequential searches in {duration:.2f} seconds")
    
    # Test 2: Concurrent searches
    logger.info("\nTest 2: Concurrent search performance...")
    
    async def concurrent_search(worker_id: int, count: int):
        """Run multiple searches concurrently."""
        for i in range(count):
            await search.search(f"worker_{worker_id}_query_{i}", limit=5)
    
    start_time = datetime.utcnow()
    workers = [concurrent_search(i, 10) for i in range(10)]
    await asyncio.gather(*workers)
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(f"✓ 100 concurrent searches (10x10) in {duration:.2f} seconds")
    
    # Test 3: Large result sets
    logger.info("\nTest 3: Large result set handling...")
    large_results = await search.search("error", limit=1000)
    logger.success(f"✓ Handled large result request (got {len(large_results)} results)")
    
    logger.info("\n📊 Stress Test Summary: All tests passed")
    return True


if __name__ == "__main__":
    """
    Script entry point with triple-mode execution.
    
    Usage:
        python hybrid_search.py          # Runs working_usage() - stable tests
        python hybrid_search.py debug    # Runs debug_function() - experimental
        python hybrid_search.py stress   # Runs stress_test() - performance tests
    """
    import sys
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        """Main async entry point."""
        if mode == "debug":
            logger.info("Running in DEBUG mode...")
            success = await debug_function()
        elif mode == "stress":
            logger.info("Running in STRESS TEST mode...")
            success = await stress_test()
        else:
            logger.info("Running in WORKING mode...")
            success = await working_usage()
        
        return success
    
    # Single asyncio.run() call
    success = asyncio.run(main())
    exit(0 if success else 1)
```

### 4. `src/arangodb/core/graph/relationship_extraction.py` - ❌ BROKEN - MOCK IMPLEMENTATION

```python
#!/usr/bin/env python3
"""
Mock relationship extraction module for Logger Agent integration.

Provides a simplified interface for extracting relationships between
log events without requiring the full graph infrastructure.

Third-party Documentation:
- ArangoDB Graphs: https://www.arangodb.com/docs/stable/graphs.html

Example Input:
    text1 = "Database connection failed"
    text2 = "Retrying database connection"

Expected Output:
    [{"type": "RETRY_OF", "confidence": 0.85, "from": text1, "to": text2}]
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

from loguru import logger

# Configure logging
logger.remove()
logger.add(
    sys.stderr,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)


# ============================================
# CORE FUNCTIONS (Outside __main__ block)
# ============================================

class RelationshipExtractor:
    """Mock relationship extractor for testing."""
    
    def __init__(self, db=None):
        """Initialize with optional database connection."""
        self.db = db
        logger.info("RelationshipExtractor mock initialized")
    
    async def extract_relationships(
        self,
        text1: str,
        text2: str,
        context: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Extract relationships between two text snippets.
        
        Args:
            text1: First text to analyze
            text2: Second text to analyze
            context: Optional context information
            
        Returns:
            List of relationships with type and confidence
        """
        relationships = []
        
        # Simple pattern matching for demonstration
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        # Check for retry patterns
        if "failed" in text1_lower and "retry" in text2_lower:
            relationships.append({
                "type": "RETRY_OF",
                "confidence": 0.85,
                "from": text1,
                "to": text2
            })
        
        # Check for error-fix patterns
        if "error" in text1_lower and ("fix" in text2_lower or "resolved" in text2_lower):
            relationships.append({
                "type": "FIXED_BY",
                "confidence": 0.90,
                "from": text1,
                "to": text2
            })
        
        # Check for cause-effect patterns
        if "caused" in text2_lower or "because" in text2_lower:
            relationships.append({
                "type": "CAUSED_BY",
                "confidence": 0.75,
                "from": text2,
                "to": text1
            })
        
        # Temporal relationship (always exists between sequential logs)
        relationships.append({
            "type": "FOLLOWED_BY",
            "confidence": 1.0,
            "from": text1,
            "to": text2
        })
        
        return relationships


def save_results(results: Dict[str, Any], output_dir: Optional[Path] = None) -> Path:
    """Save results to JSON file."""
    if output_dir is None:
        output_dir = Path(__file__).parent / "tmp" / "responses"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{Path(__file__).stem}_results_{timestamp}.json"
    output_path = output_dir / filename
    
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, sort_keys=True)
    
    logger.info(f"Results saved to: {output_path}")
    return output_path


# ============================================
# USAGE EXAMPLES (Inside __main__ block)
# ============================================

async def working_usage():
    """
    Demonstrate RelationshipExtractor functionality.
    
    Shows how to extract relationships between log messages.
    """
    logger.info("=== Running Working Usage Examples ===")
    
    extractor = RelationshipExtractor()
    
    # Test 1: Error and retry relationship
    logger.info("\nTest 1: Error-Retry relationship...")
    rels = await extractor.extract_relationships(
        "Database connection failed with timeout",
        "Retrying database connection attempt 1"
    )
    
    logger.info(f"Found {len(rels)} relationships:")
    for rel in rels:
        logger.info(f"  - {rel['type']} (confidence: {rel['confidence']})")
    
    assert any(r["type"] == "RETRY_OF" for r in rels), "Should find RETRY_OF relationship"
    
    # Test 2: Error and fix relationship
    logger.info("\nTest 2: Error-Fix relationship...")
    rels = await extractor.extract_relationships(
        "Critical error: Memory overflow detected",
        "Memory issue fixed by clearing cache"
    )
    
    fix_rels = [r for r in rels if r["type"] == "FIXED_BY"]
    assert len(fix_rels) > 0, "Should find FIXED_BY relationship"
    logger.success(f"✓ Found FIXED_BY relationship with confidence {fix_rels[0]['confidence']}")
    
    # Test 3: Multiple relationships
    logger.info("\nTest 3: Complex relationship extraction...")
    test_pairs = [
        ("Service crashed unexpectedly", "Service restarted automatically"),
        ("High CPU usage detected", "Process killed due to high CPU"),
        ("Configuration error found", "Configuration error resolved")
    ]
    
    all_relationships = []
    for text1, text2 in test_pairs:
        rels = await extractor.extract_relationships(text1, text2)
        all_relationships.extend(rels)
        logger.info(f"  {text1[:30]}... -> {text2[:30]}... : {len(rels)} relationships")
    
    # Save results
    save_results({
        "test_pairs": test_pairs,
        "relationships": all_relationships,
        "timestamp": datetime.utcnow().isoformat()
    })
    
    logger.success("✓ All relationship extraction tests passed")
    return True


async def debug_function():
    """
    Debug function for testing edge cases in relationship extraction.
    """
    logger.info("=== Running Debug Function ===")
    
    extractor = RelationshipExtractor()
    
    # Test empty strings
    logger.info("Testing empty strings...")
    rels = await extractor.extract_relationships("", "")
    logger.info(f"Empty strings: {len(rels)} relationships")
    
    # Test identical strings
    logger.info("\nTesting identical strings...")
    identical_text = "Same message repeated"
    rels = await extractor.extract_relationships(identical_text, identical_text)
    logger.info(f"Identical strings: {len(rels)} relationships")
    
    # Test very long strings
    logger.info("\nTesting very long strings...")
    long_text1 = "Error: " + "A" * 1000
    long_text2 = "Fixed: " + "B" * 1000
    rels = await extractor.extract_relationships(long_text1, long_text2)
    logger.info(f"Long strings: {len(rels)} relationships")
    
    # Test special characters
    logger.info("\nTesting special characters...")
    special1 = "Error: @#$%^&*() failed"
    special2 = "Retry: !@#$%^&*() attempt"
    rels = await extractor.extract_relationships(special1, special2)
    logger.info(f"Special chars: {len(rels)} relationships")
    
    # Test with context
    logger.info("\nTesting with context...")
    context = {"execution_id": "test_123", "timestamp_diff": 5.2}
    rels = await extractor.extract_relationships(
        "Operation failed",
        "Operation retried",
        context=context
    )
    logger.info(f"With context: {len(rels)} relationships")
    
    return True


async def stress_test():
    """
    Stress test relationship extraction with high volume.
    """
    logger.info("=== Running Stress Tests ===")
    
    extractor = RelationshipExtractor()
    
    # Test 1: Many sequential extractions
    logger.info("Test 1: Sequential extraction performance...")
    start_time = datetime.utcnow()
    
    total_relationships = 0
    for i in range(500):
        rels = await extractor.extract_relationships(
            f"Log message {i}: Operation in progress",
            f"Log message {i+1}: Operation completed"
        )
        total_relationships += len(rels)
        
        if i % 100 == 0:
            logger.info(f"  Processed {i} pairs")
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(
        f"✓ Extracted {total_relationships} relationships from 500 pairs "
        f"in {duration:.2f} seconds"
    )
    
    # Test 2: Concurrent extractions
    logger.info("\nTest 2: Concurrent extraction performance...")
    
    async def extract_batch(batch_id: int, count: int):
        """Extract relationships for a batch of log pairs."""
        batch_rels = 0
        for i in range(count):
            rels = await extractor.extract_relationships(
                f"Batch {batch_id} error {i}",
                f"Batch {batch_id} recovery {i}"
            )
            batch_rels += len(rels)
        return batch_rels
    
    start_time = datetime.utcnow()
    tasks = [extract_batch(i, 50) for i in range(10)]
    batch_results = await asyncio.gather(*tasks)
    total_concurrent = sum(batch_results)
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(
        f"✓ Extracted {total_concurrent} relationships concurrently "
        f"(10 batches x 50 pairs) in {duration:.2f} seconds"
    )
    
    # Test 3: Complex pattern matching
    logger.info("\nTest 3: Complex pattern performance...")
    complex_patterns = [
        ("System error: Database connection pool exhausted after 300 attempts",
         "Recovery initiated: Restarting connection pool and clearing stale connections"),
        ("Performance degradation: Response time increased from 100ms to 5000ms",
         "Optimization applied: Enabled query caching, response time reduced to 150ms"),
        ("Security alert: Multiple failed authentication attempts from IP 192.168.1.100",
         "Security response: IP blocked and alert sent to administrators")
    ]
    
    start_time = datetime.utcnow()
    for _ in range(100):
        for text1, text2 in complex_patterns:
            await extractor.extract_relationships(text1, text2)
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(f"✓ Complex pattern extraction (300 ops) in {duration:.2f} seconds")
    
    logger.info("\n📊 Stress Test Summary: All tests passed")
    return True


if __name__ == "__main__":
    """
    Script entry point with triple-mode execution.
    
    Usage:
        python relationship_extraction.py          # Runs working_usage()
        python relationship_extraction.py debug    # Runs debug_function()
        python relationship_extraction.py stress   # Runs stress_test()
    """
    import sys
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        """Main async entry point."""
        if mode == "debug":
            logger.info("Running in DEBUG mode...")
            success = await debug_function()
        elif mode == "stress":
            logger.info("Running in STRESS TEST mode...")
            success = await stress_test()
        else:
            logger.info("Running in WORKING mode...")
            success = await working_usage()
        
        return success
    
    # Single asyncio.run() call
    success = asyncio.run(main())
    exit(0 if success else 1)
```

### 5. `src/arangodb/core/memory/memory_agent.py` - ❌ BROKEN - MOCK IMPLEMENTATION

```python
#!/usr/bin/env python3
"""
Mock memory agent module for Logger Agent integration.

Provides a simplified interface for storing and retrieving memories
without requiring the full memory infrastructure.

Third-party Documentation:
- ArangoDB Documents: https://www.arangodb.com/docs/stable/data-modeling-documents.html

Example Input:
    content = "Discovered optimal batch size is 100"
    memory_type = "learning"
    metadata = {"confidence": 0.9, "context": "data_processing"}

Expected Output:
    {"_id": "memories/123", "content": "...", "type": "learning", "timestamp": "..."}
"""

import asyncio
import json
import sys
import uuid
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

from loguru import logger

# Configure logging
logger.remove()
logger.add(
    sys.stderr,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)


# ============================================
# CORE FUNCTIONS (Outside __main__ block)
# ============================================

class MemoryAgent:
    """Mock memory agent for testing."""
    
    def __init__(self, db=None):
        """Initialize with optional database connection."""
        self.db = db
        self.memories = []  # In-memory storage for mock
        logger.info("MemoryAgent mock initialized")
    
    async def add_memory(
        self,
        content: str,
        memory_type: str = "general",
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Add a new memory entry.
        
        Args:
            content: Memory content
            memory_type: Type of memory (learning, observation, etc.)
            metadata: Additional metadata
            
        Returns:
            Created memory document
        """
        memory_id = str(uuid.uuid4())[:8]
        memory = {
            "_id": f"memories/{memory_id}",
            "_key": memory_id,
            "content": content,
            "type": memory_type,
            "timestamp": datetime.utcnow().isoformat(),
            "metadata": metadata or {}
        }
        
        self.memories.append(memory)
        logger.info(f"Added {memory_type} memory: {content[:50]}...")
        
        return memory
    
    async def search_memories(
        self,
        query: str,
        memory_type: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Search for memories matching query.
        
        Args:
            query: Search query
            memory_type: Optional filter by type
            limit: Maximum results
            
        Returns:
            List of matching memories
        """
        results = []
        query_lower = query.lower()
        
        for memory in self.memories:
            # Simple keyword matching
            if query_lower in memory["content"].lower():
                if memory_type is None or memory["type"] == memory_type:
                    results.append(memory)
                    if len(results) >= limit:
                        break
        
        logger.info(f"Found {len(results)} memories matching '{query}'")
        return results
    
    async def get_recent_memories(
        self,
        memory_type: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Get recent memories, optionally filtered by type.
        
        Args:
            memory_type: Optional filter by type
            limit: Maximum results
            
        Returns:
            List of recent memories
        """
        # Filter by type if specified
        memories = self.memories
        if memory_type:
            memories = [m for m in memories if m["type"] == memory_type]
        
        # Sort by timestamp (newest first) and limit
        memories.sort(key=lambda m: m["timestamp"], reverse=True)
        
        return memories[:limit]


def save_results(results: Dict[str, Any], output_dir: Optional[Path] = None) -> Path:
    """Save results to JSON file."""
    if output_dir is None:
        output_dir = Path(__file__).parent / "tmp" / "responses"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{Path(__file__).stem}_results_{timestamp}.json"
    output_path = output_dir / filename
    
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, sort_keys=True)
    
    logger.info(f"Results saved to: {output_path}")
    return output_path


# ============================================
# USAGE EXAMPLES (Inside __main__ block)
# ============================================

async def working_usage():
    """
    Demonstrate MemoryAgent functionality.
    
    Shows how to store and retrieve various types of memories.
    """
    logger.info("=== Running Working Usage Examples ===")
    
    agent = MemoryAgent()
    
    # Test 1: Add different types of memories
    logger.info("\nTest 1: Adding various memory types...")
    
    # Add a learning
    learning = await agent.add_memory(
        "Batch size of 100 is optimal for datasets under 10GB",
        memory_type="learning",
        metadata={"confidence": 0.9, "context": "optimization"}
    )
    logger.success(f"✓ Added learning: {learning['_id']}")
    
    # Add an observation
    observation = await agent.add_memory(
        "System performance degrades when memory usage exceeds 80%",
        memory_type="observation",
        metadata={"severity": "warning", "threshold": 0.8}
    )
    logger.success(f"✓ Added observation: {observation['_id']}")
    
    # Add an error pattern
    error_pattern = await agent.add_memory(
        "ConnectionError often occurs after 5 minutes of idle time",
        memory_type="error_pattern",
        metadata={"frequency": "common", "timeout": 300}
    )
    logger.success(f"✓ Added error pattern: {error_pattern['_id']}")
    
    # Test 2: Search memories
    logger.info("\nTest 2: Searching memories...")
    
    # Search for batch-related memories
    batch_memories = await agent.search_memories("batch", limit=5)
    assert len(batch_memories) > 0, "Should find batch-related memories"
    logger.info(f"Found {len(batch_memories)} memories about 'batch'")
    
    # Search for specific type
    learnings = await agent.search_memories("", memory_type="learning", limit=10)
    logger.info(f"Found {len(learnings)} learning memories")
    
    # Test 3: Get recent memories
    logger.info("\nTest 3: Getting recent memories...")
    
    recent_all = await agent.get_recent_memories(limit=5)
    logger.info(f"Got {len(recent_all)} recent memories (all types)")
    
    recent_observations = await agent.get_recent_memories(
        memory_type="observation",
        limit=3
    )
    logger.info(f"Got {len(recent_observations)} recent observations")
    
    # Save results
    save_results({
        "total_memories": len(agent.memories),
        "sample_learning": learning,
        "search_results": batch_memories,
        "recent_memories": recent_all
    })
    
    logger.success("✓ All memory tests passed")
    return True


async def debug_function():
    """
    Debug function for testing memory edge cases.
    """
    logger.info("=== Running Debug Function ===")
    
    agent = MemoryAgent()
    
    # Test empty content
    logger.info("Testing empty content...")
    empty_memory = await agent.add_memory("", memory_type="test")
    logger.info(f"Empty memory created: {empty_memory['_id']}")
    
    # Test very long content
    logger.info("\nTesting very long content...")
    long_content = "A" * 10000
    long_memory = await agent.add_memory(
        long_content,
        memory_type="stress_test",
        metadata={"size": len(long_content)}
    )
    logger.info(f"Long memory created: {long_memory['_id']}")
    
    # Test special characters
    logger.info("\nTesting special characters...")
    special_content = "Special chars: @#$%^&*()_+{}[]|\\:;\"'<>,.?/"
    special_memory = await agent.add_memory(special_content, memory_type="special")
    
    # Test Unicode
    logger.info("\nTesting Unicode content...")
    unicode_content = "Unicode test: 你好世界 🌍 مرحبا بالعالم"
    unicode_memory = await agent.add_memory(unicode_content, memory_type="unicode")
    
    # Test search with special queries
    logger.info("\nTesting special search queries...")
    
    # Empty search
    empty_results = await agent.search_memories("")
    logger.info(f"Empty search returned {len(empty_results)} results")
    
    # Special char search
    special_results = await agent.search_memories("@#$%")
    logger.info(f"Special char search returned {len(special_results)} results")
    
    # Case sensitivity
    await agent.add_memory("UPPERCASE content", memory_type="case_test")
    case_results = await agent.search_memories("uppercase")
    logger.info(f"Case-insensitive search returned {len(case_results)} results")
    
    return True


async def stress_test():
    """
    Stress test memory operations with high volume.
    """
    logger.info("=== Running Stress Tests ===")
    
    agent = MemoryAgent()
    
    # Test 1: Add many memories
    logger.info("Test 1: Adding many memories...")
    start_time = datetime.utcnow()
    
    memory_types = ["learning", "observation", "error_pattern", "optimization", "debug"]
    
    for i in range(1000):
        await agent.add_memory(
            f"Memory {i}: Important information about process {i % 10}",
            memory_type=memory_types[i % len(memory_types)],
            metadata={"index": i, "batch": i // 100}
        )
        
        if i % 200 == 0:
            logger.info(f"  Added {i} memories")
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(f"✓ Added 1000 memories in {duration:.2f} seconds")
    
    # Test 2: Search performance
    logger.info("\nTest 2: Search performance...")
    
    search_terms = ["process", "information", "important", "0", "memory"]
    total_results = 0
    
    start_time = datetime.utcnow()
    for term in search_terms:
        results = await agent.search_memories(term, limit=100)
        total_results += len(results)
        logger.info(f"  '{term}': {len(results)} results")
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(
        f"✓ Searched {len(search_terms)} terms, found {total_results} total results "
        f"in {duration:.2f} seconds"
    )
    
    # Test 3: Concurrent operations
    logger.info("\nTest 3: Concurrent memory operations...")
    
    async def memory_worker(worker_id: int, operation_count: int):
        """Perform memory operations concurrently."""
        for i in range(operation_count):
            if i % 2 == 0:
                # Add memory
                await agent.add_memory(
                    f"Worker {worker_id} memory {i}",
                    memory_type="concurrent_test"
                )
            else:
                # Search memory
                await agent.search_memories(f"Worker {worker_id}")
    
    start_time = datetime.utcnow()
    workers = [memory_worker(i, 20) for i in range(10)]
    await asyncio.gather(*workers)
    
    duration = (datetime.utcnow() - start_time).total_seconds()
    logger.success(
        f"✓ Completed 200 concurrent operations (10 workers x 20 ops) "
        f"in {duration:.2f} seconds"
    )
    
    # Final statistics
    logger.info(f"\nFinal memory count: {len(agent.memories)}")
    
    type_counts = {}
    for memory in agent.memories:
        mem_type = memory["type"]
        type_counts[mem_type] = type_counts.get(mem_type, 0) + 1
    
    logger.info("Memory type distribution:")
    for mem_type, count in type_counts.items():
        logger.info(f"  {mem_type}: {count}")
    
    logger.info("\n📊 Stress Test Summary: All tests passed")
    return True


if __name__ == "__main__":
    """
    Script entry point with triple-mode execution.
    
    Usage:
        python memory_agent.py          # Runs working_usage() - stable tests
        python memory_agent.py debug    # Runs debug_function() - experimental
        python memory_agent.py stress   # Runs stress_test() - performance tests
    """
    import sys
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        """Main async entry point."""
        if mode == "debug":
            logger.info("Running in DEBUG mode...")
            success = await debug_function()
        elif mode == "stress":
            logger.info("Running in STRESS TEST mode...")
            success = await stress_test()
        else:
            logger.info("Running in WORKING mode...")
            success = await working_usage()
        
        return success
    
    # Single asyncio.run() call
    success = asyncio.run(main())
    exit(0 if success else 1)
```

### 6. `src/arango_log_sink.py` - ❌ NEEDS FIXES

```python
#!/usr/bin/env python3
"""
arango_log_sink.py - Custom loguru sink for async ArangoDB writes

Implements non-blocking log ingestion with batching, buffering,
and automatic retry mechanisms.
"""

import asyncio
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from collections import deque
import aiofiles
import uvloop
import sys

from aioarango import ArangoClient
from loguru import logger
from tenacity import retry, stop_after_attempt, wait_exponential
import psutil

from utils.log_utils import log_safe_results

# Set uvloop as the event loop
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())


class ArangoLogSink:
    """Async sink for loguru that writes to ArangoDB with buffering."""
    
    def __init__(
        self,
        db_config: Dict[str, str],
        batch_size: int = 100,
        flush_interval: float = 2.0,
        buffer_dir: Path = Path("/tmp/logger_agent_buffer"),
        max_buffer_size_mb: int = 1000
    ):
        self.db_config = db_config
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.buffer_dir = buffer_dir
        self.max_buffer_size_mb = max_buffer_size_mb
        
        # Ensure buffer directory and a subdirectory for failed buffers exist
        self.buffer_dir.mkdir(parents=True, exist_ok=True)
        (self.buffer_dir / "_failed").mkdir(exist_ok=True) # Directory for problematic files
        
        # Log queue and batch
        self.log_queue: asyncio.Queue = asyncio.Queue(maxsize=10000)
        self.current_batch: deque = deque(maxlen=batch_size)
        
        # Database connection
        self.client = None
        self.db = None
        self.collection = None
        self.connected = False
        
        # Monitoring
        self.stats = {
            "total_logs": 0,
            "successful_writes": 0,
            "failed_writes": 0,
            "buffered_logs": 0,
            "last_error": None
        }
        
        # Background tasks
        self.consumer_task = None
        self.flush_task = None
        self.monitor_task = None
        
    async def connect(self) -> bool:
        """Establish connection to ArangoDB."""
        try:
            self.client = ArangoClient(hosts=self.db_config["url"])
            self.db = await self.client.db(
                self.db_config["database"],
                username=self.db_config["username"],
                password=self.db_config["password"]
            )
            self.collection = self.db.collection("log_events")
            
            # Test connection
            await self.db.version()
            self.connected = True
            logger.info("Connected to ArangoDB")
            return True
            
        except Exception as e:
            logger.error(f"Failed to connect to ArangoDB: {e}")
            self.connected = False
            self.stats["last_error"] = str(e)
            return False
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def write_batch_to_db(self, batch: List[Dict[str, Any]]) -> bool:
        """Write a batch of logs to ArangoDB with retry logic."""
        if not self.connected:
            if not await self.connect():
                raise Exception("Database connection failed") # Raise to trigger tenacity retry
        
        try:
            safe_batch = log_safe_results(batch) # Use log_safe_results here
            
            result = await self.collection.insert_many(safe_batch)
            self.stats["successful_writes"] += len(batch)
            return True
            
        except Exception as e:
            logger.error(f"Failed to write batch: {e}")
            self.stats["failed_writes"] += len(batch)
            self.stats["last_error"] = str(e)
            self.connected = False  # Mark as disconnected for next attempt
            raise # Re-raise to trigger tenacity retry
    
    async def buffer_to_disk(self, logs: List[Dict[str, Any]]) -> None:
        """Buffer logs to disk when database is unavailable."""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S_%f")
        buffer_file = self.buffer_dir / f"buffer_{timestamp}.jsonl"
        
        try:
            async with aiofiles.open(buffer_file, 'w') as f:
                for log in logs:
                    await f.write(json.dumps(log) + '\n')
            
            self.stats["buffered_logs"] += len(logs)
            logger.warning(f"Buffered {len(logs)} logs to {buffer_file}")
            
            # Check buffer size and clean if needed
            await self.check_buffer_size()
        except Exception as e:
            logger.critical(f"Failed to write logs to disk buffer {buffer_file}: {e}")
            # At this point, logs are effectively lost if disk buffering fails.
            # This should ideally trigger an external alert.
    
    async def check_buffer_size(self) -> None:
        """Monitor buffer directory size and clean old files if needed."""
        if not self.buffer_dir.exists():
            return
            
        total_size = sum(f.stat().st_size for f in self.buffer_dir.glob("*.jsonl"))
        total_size_mb = total_size / (1024 * 1024)
        
        if total_size_mb > self.max_buffer_size_mb:
            # Remove oldest files
            files = sorted(self.buffer_dir.glob("*.jsonl"), key=lambda f: f.stat().st_mtime)
            
            while total_size_mb > self.max_buffer_size_mb * 0.8 and files:
                oldest = files.pop(0)
                try:
                    size = oldest.stat().st_size
                    oldest.unlink()
                    total_size_mb -= size / (1024 * 1024)
                    logger.warning(f"Removed old buffer file: {oldest.name} to stay within buffer limits.")
                except OSError as e:
                    logger.error(f"Failed to delete old buffer file {oldest.name}: {e}")
    
    async def process_buffered_logs(self) -> None:
        """Process buffered logs when connection is restored."""
        if not self.connected or not self.buffer_dir.exists():
            return
        
        buffer_files = sorted(self.buffer_dir.glob("*.jsonl"))
        failed_buffer_dir = self.buffer_dir / "_failed"
        failed_buffer_dir.mkdir(exist_ok=True) # Ensure it exists
        
        for buffer_file in buffer_files:
            try:
                logs = []
                async with aiofiles.open(buffer_file, 'r') as f:
                    async for line in f:
                        logs.append(json.loads(line.strip()))
                
                if logs:
                    # Process in batches
                    for i in range(0, len(logs), self.batch_size):
                        batch = logs[i:i + self.batch_size]
                        try:
                            await self.write_batch_to_db(batch)
                        except Exception as inner_e:
                            logger.error(f"Persistent failure to write batch from buffered file {buffer_file.name}: {inner_e}. Re-buffering this part of the file.")
                            # If write_batch_to_db fails even after its retries, re-buffer the failed batch
                            await self.buffer_to_disk(batch) 
                            # Move the original problematic file to _failed and stop processing this file
                            buffer_file.rename(failed_buffer_dir / buffer_file.name)
                            logger.warning(f"Moved problematic buffer file to {failed_buffer_dir.name}: {buffer_file.name}")
                            break # Stop processing current file, move to next
                    else: # This 'else' runs if the for loop completed without a 'break'
                        # If all batches from this file were successfully written, remove it
                        buffer_file.unlink()
                        logger.info(f"Processed and removed buffered file: {buffer_file.name}")
                else:
                    # If file is empty, remove it
                    buffer_file.unlink()
                    logger.info(f"Removed empty buffered file: {buffer_file.name}")
                
            except Exception as e:
                logger.error(f"Failed to process or parse buffer file {buffer_file.name}: {e}")
                # Move problematic file to a 'failed' sub-directory for manual inspection
                try:
                    buffer_file.rename(failed_buffer_dir / buffer_file.name)
                    logger.warning(f"Moved problematic buffer file to {failed_buffer_dir.name}: {buffer_file.name}")
                except Exception as move_e:
                    logger.critical(f"Could not move problematic file {buffer_file.name} to _failed directory: {move_e}")
                # Continue to next file, don't break
                continue
    
    async def log_consumer(self) -> None:
        """Consume logs from queue and batch them."""
        while True:
            try:
                # Get log from queue with timeout
                log_data = await asyncio.wait_for(
                    self.log_queue.get(),
                    timeout=self.flush_interval
                )
                
                self.current_batch.append(log_data)
                self.stats["total_logs"] += 1
                
                # Flush if batch is full
                if len(self.current_batch) >= self.batch_size:
                    await self.flush_batch()
                
            except asyncio.TimeoutError:
                # Flush on timeout
                if self.current_batch:
                    await self.flush_batch()
            except Exception as e:
                logger.error(f"Error in log consumer: {e}")
    
    async def flush_batch(self) -> None:
        """Flush current batch to database or disk buffer."""
        if not self.current_batch:
            return
        
        batch = list(self.current_batch)
        self.current_batch.clear()
        
        try:
            await self.write_batch_to_db(batch)
            
            # Try to process buffered logs after successful write
            await self.process_buffered_logs()
            
        except Exception: # Catch any exception from write_batch_to_db, which already logs failures
            # Buffer to disk on failure
            await self.buffer_to_disk(batch)
    
    async def periodic_flush(self) -> None:
        """Periodically flush logs."""
        while True:
            await asyncio.sleep(self.flush_interval)
            await self.flush_batch()
    
    async def monitor_performance(self) -> None:
        """Monitor sink performance and alert on issues."""
        alert_threshold = int(os.getenv("ALERT_LOG_FAILURE_THRESHOLD", "10"))
        monitoring_interval = int(os.getenv("MONITORING_INTERVAL", "60"))
        
        while True:
            await asyncio.sleep(monitoring_interval)
            
            # Calculate failure rate
            total = self.stats["successful_writes"] + self.stats["failed_writes"]
            if total > 0:
                failure_rate = self.stats["failed_writes"] / total * 100
                
                if failure_rate > alert_threshold:
                    logger.critical(
                        f"High log failure rate: {failure_rate:.1f}% "
                        f"({self.stats['failed_writes']}/{total} failed)"
                    )
            
            # Check memory usage
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 80:
                logger.warning(f"High memory usage: {memory_percent}%")
            
            # Log stats
            logger.info(f"Log sink stats: {self.stats}")
    
    async def start(self) -> None:
        """Start background tasks."""
        await self.connect()
        
        self.consumer_task = asyncio.create_task(self.log_consumer())
        self.flush_task = asyncio.create_task(self.periodic_flush())
        
        if os.getenv("ENABLE_MONITORING", "true").lower() == "true":
            self.monitor_task = asyncio.create_task(self.monitor_performance())
    
    async def stop(self) -> None:
        """Stop background tasks and flush remaining logs."""
        # Cancel tasks
        for task in [self.consumer_task, self.flush_task, self.monitor_task]:
            if task:
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass
        
        # Final flush
        await self.flush_batch()
        
        # Close database connection
        if self.client:
            await self.client.close()
    
    def write(self, message: Dict[str, Any]) -> None:
        """Synchronous write method for loguru compatibility."""
        # Parse loguru message
        record = message.record
        
        # Extract file name without path, and function from record
        script_name = Path(record["file"].name).stem
        function_name = record["function"]
        
        log_data = {
            "timestamp": record["time"].isoformat(),
            "level": record["level"].name,
            "message": record["message"],
            "execution_id": record["extra"].get("execution_id", "unknown"),
            "script_name": record["extra"].get("script_name", script_name),
            "function_name": function_name,
            "file_path": record["file"].path,
            "line_number": record["line"],
            "extra_data": record["extra"],
            "tags": record["extra"].get("tags", [])
        }
        
        # Add to queue (non-blocking)
        try:
            self.log_queue.put_nowait(log_data)
        except asyncio.QueueFull:
            # Queue is full, log to stderr as fallback
            # IMPORTANT: Using logger.bind(skip_sink=True) for fallback to avoid infinite loop
            logger.bind(skip_sink=True).error(
                f"Log queue full, dropping log: {log_data['message'][:100]}... "
                f"(Total buffered: {self.stats['buffered_logs']})"
            )


# Global sink instance
_sink_instance: Optional[ArangoLogSink] = None


def get_arango_sink() -> ArangoLogSink:
    """
    Get or create the global ArangoDB sink instance.
    
    NOTE: This function implicitly starts the sink's background tasks
    (consumer, flusher, monitor) the first time it is called by creating
    an `asyncio.Task`. In a larger, more structured application, it might be
    preferable to explicitly manage the sink's lifecycle (e.g., calling
    `sink.start()` and `sink.stop()` during application boot/shutdown).
    For standalone scripts using `asyncio.run()`, this pattern is acceptable.
    """
    global _sink_instance
    
    if _sink_instance is None:
        db_config = {
            "url": os.getenv("ARANGO_URL", "http://localhost:8529"),
            "database": os.getenv("ARANGO_DATABASE", "script_logs"),
            "username": os.getenv("ARANGO_USERNAME", "root"),
            "password": os.getenv("ARANGO_PASSWORD", "openSesame")
        }
        
        _sink_instance = ArangoLogSink(
            db_config=db_config,
            batch_size=int(os.getenv("LOG_BATCH_SIZE", "200")),
            flush_interval=float(os.getenv("LOG_FLUSH_INTERVAL", "2")),
            buffer_dir=Path(os.getenv("LOG_BUFFER_DIR", "/tmp/logger_agent_buffer")),
            max_buffer_size_mb=int(os.getenv("LOG_MAX_BUFFER_SIZE_MB", "1000"))
        )
        
        # Start sink in background as an asyncio task.
        # This task will run as long as the event loop is active.
        asyncio.create_task(_sink_instance.start())
    
    return _sink_instance


async def working_usage():
    """Test the ArangoDB sink with sample logs."""
    logger.info("=== Testing ArangoDB Sink ===")
    
    # Configure logger with our sink
    sink = get_arango_sink()
    logger.add(sink.write, enqueue=True)
    
    # Generate test logs
    execution_id = f"test_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
    
    logger.bind(execution_id=execution_id, tags=["test", "sink"]).info("Test log 1")
    logger.bind(execution_id=execution_id, tags=["test"]).warning("Test warning")
    logger.bind(execution_id=execution_id).error("Test error")
    
    # Wait for logs to be written
    await asyncio.sleep(3)
    
    # Check stats
    logger.info(f"Sink stats: {sink.stats}")
    
    return True


async def debug_function():
    """Debug sink behavior under various conditions."""
    logger.info("=== Debug Mode: Testing Edge Cases ===")
    
    sink = get_arango_sink()
    logger.add(sink.write, enqueue=True)
    
    # Test 1: High volume logging
    logger.info("Test 1: High volume")
    for i in range(1000):
        logger.bind(test_id=f"volume_{i}").debug(f"High volume test {i}")
    
    await asyncio.sleep(5)
    logger.info(f"After high volume: {sink.stats}")
    
    # Test 2: Large messages
    logger.info("Test 2: Large messages")
    large_data = {"data": "x" * 10000, "array": list(range(1000))}
    logger.bind(extra_data=large_data).info("Large message test")
    
    await asyncio.sleep(2)
    
    # Test 3: Connection failure simulation
    logger.info("Test 3: Simulating connection failure (logs will buffer to disk)")
    
    # Temporarily disable connection
    sink.connected = False
    
    for i in range(10):
        logger.bind(test="failover").error(f"Failover test {i}")
    
    await asyncio.sleep(3) # Give time for logs to be put in queue and attempt write
    logger.info(f"After failover (buffered logs might not show up immediately in DB): {sink.stats}")
    
    # Re-enable connection and process buffered files
    logger.info("Re-enabling connection to process buffered logs...")
    await sink.connect() # Attempt to reconnect
    await asyncio.sleep(5) # Give time for processing buffered logs
    
    logger.info(f"After reconnect and processing: {sink.stats}")

    # Check buffer files
    buffer_files = list(sink.buffer_dir.glob("*.jsonl"))
    failed_buffer_files = list((sink.buffer_dir / "_failed").glob("*.jsonl"))
    logger.info(f"Remaining buffer files: {len(buffer_files)}")
    logger.info(f"Failed buffer files (quarantined): {len(failed_buffer_files)}")
    
    return True


if __name__ == "__main__":
    import sys
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # Configure root logger with a basic console handler as a fallback.
    # For standalone scripts, logger.remove() is used to ensure a clean slate,
    # preventing duplicate output if run multiple times in a session.
    # In a larger, long-running application, logging setup should be centralized
    # and might not involve calling logger.remove() indiscriminately.
    logger.remove() 
    logger.add(sys.stderr, level="INFO")

    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        success = False
        try:
            if mode == "debug":
                logger.info("Running in DEBUG mode...")
                success = await debug_function()
            else:
                logger.info("Running in WORKING mode...")
                success = await working_usage()
            
        finally:
            # Cleanup
            sink = get_arango_sink()
            if sink:
                logger.info("Stopping ArangoLogSink...")
                await sink.stop()
                logger.info("ArangoLogSink stopped.")
        
        return success
    
    success = asyncio.run(main())
    exit(0 if success else 1)
```

### 7. `src/agent_log_manager.py` - ❌ NEEDS FIXES

```python
#!/usr/bin/env python3
"""
agent_log_manager.py - Unified API for agent logging and introspection

Provides a singleton interface for agents to log, query, and analyze
their execution history using ArangoDB backend.
"""

import asyncio
import json
import uuid
import os 
import socket 
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from contextlib import asynccontextmanager

from aioarango import ArangoClient
from loguru import logger
# numpy is imported but not used directly in the provided snippet's logic, keeping it for compatibility if other parts of the project use it.
import numpy as np 
from tenacity import retry, stop_after_attempt, wait_exponential

# Import from existing modules (MOCK implementations)
from arangodb.core.search.hybrid_search import HybridSearch
from arangodb.core.graph.relationship_extraction import RelationshipExtractor
from arangodb.core.memory.memory_agent import MemoryAgent
from utils.log_utils import truncate_large_value 


class AgentLogManager:
    """Singleton manager for agent logging operations."""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.db = None
            self.client = None
            self.execution_context = {}
            self.current_execution_id = None
            
            # Integration modules
            self.hybrid_search = None
            self.relationship_extractor = None
            self.memory_agent = None
            
            self._initialized = True
    
    async def initialize(self, db_config: Dict[str, str]) -> None:
        """Initialize database connection and integration modules."""
        try:
            self.client = ArangoClient(hosts=db_config["url"])
            self.db = await self.client.db(
                db_config["database"],
                username=db_config["username"],
                password=db_config["password"]
            )
            
            # Initialize integration modules
            self.hybrid_search = HybridSearch(self.db)
            self.relationship_extractor = RelationshipExtractor(self.db)
            self.memory_agent = MemoryAgent(self.db)
            
            logger.info("AgentLogManager initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize AgentLogManager: {e}")
            raise
    
    def generate_execution_id(self, script_name: str) -> str:
        """Generate unique execution ID."""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        return f"{script_name}_{timestamp}_{unique_id}"
    
    @asynccontextmanager
    async def script_execution(self, script_name: str, metadata: Optional[Dict] = None):
        """
        Context manager for script execution tracking.
        Yields a Loguru logger instance with `execution_id` and `script_name` bound.
        """
        execution_id = self.generate_execution_id(script_name)
        self.current_execution_id = execution_id
        
        # Start script run
        await self.start_run(script_name, execution_id, metadata)
        
        try:
            # Bind execution context to logger (using an existing loguru feature)
            logger_with_context = logger.bind(
                execution_id=execution_id,
                script_name=script_name
            )
            
            yield logger_with_context
            
            # Mark as successful
            await self.end_run(execution_id, "success")
            
        except Exception as e:
            # Mark as failed
            logger.error(f"Script {script_name} failed with error: {e}", 
                         execution_id=execution_id, script_name=script_name)
            await self.end_run(execution_id, "failed", str(e))
            raise
        
        finally:
            self.current_execution_id = None
    
    async def start_run(
        self,
        script_name: str,
        execution_id: str,
        metadata: Optional[Dict] = None
    ) -> None:
        """Record script execution start."""
        doc = {
            "execution_id": execution_id,
            "script_name": script_name,
            "start_time": datetime.utcnow().isoformat(),
            "status": "running",
            "metadata": metadata or {},
            "pid": os.getpid(),
            "hostname": socket.gethostname()
        }
        
        try:
            await self.db.collection("script_runs").insert(doc)
            logger.info(f"Started script run: {execution_id}")
        except Exception as e:
            logger.error(f"Failed to record script start: {e}")
    
    async def end_run(
        self,
        execution_id: str,
        status: str = "success",
        error: Optional[str] = None
    ) -> None:
        """Record script execution end."""
        update_doc = {
            "end_time": datetime.utcnow().isoformat(),
            "status": status,
            "duration_seconds": None  # Will calculate from start_time
        }
        
        if error:
            update_doc["error"] = truncate_large_value(error, max_str_len=1000)
        
        try:
            # Get start time to calculate duration
            run_doc = await self.db.collection("script_runs").get({"execution_id": execution_id})
            if run_doc:
                start_time = datetime.fromisoformat(run_doc["start_time"])
                end_time = datetime.fromisoformat(update_doc["end_time"])
                update_doc["duration_seconds"] = (end_time - start_time).total_seconds()
            
            await self.db.collection("script_runs").update_match(
                {"execution_id": execution_id},
                update_doc
            )
            logger.info(f"Ended script run: {execution_id} ({status})")
            
        except Exception as e:
            logger.error(f"Failed to record script end: {e}")
    
    async def query_logs(
        self,
        aql_query: str,
        bind_vars: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Execute custom AQL query on logs."""
        try:
            cursor = await self.db.aql.execute(
                aql_query,
                bind_vars=bind_vars or {},
                batch_size=100
            )
            
            results = []
            async for doc in cursor:
                results.append(doc)
            
            return results
            
        except Exception as e:
            logger.error(f"AQL query failed: {e}")
            raise
    
    async def search_logs(
        self,
        query: str,
        execution_id: Optional[str] = None,
        level: Optional[str] = None,
        time_range: Optional[Dict[str, datetime]] = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Search logs with multiple filters."""
        # Build AQL query
        filters = []
        bind_vars = {"query": query, "limit": limit}
        
        if execution_id:
            filters.append("doc.execution_id == @execution_id")
            bind_vars["execution_id"] = execution_id
        
        if level:
            filters.append("doc.level == @level")
            bind_vars["level"] = level
        
        if time_range:
            if "start" in time_range:
                filters.append("doc.timestamp >= @start_time")
                bind_vars["start_time"] = time_range["start"].isoformat()
            if "end" in time_range:
                filters.append("doc.timestamp <= @end_time")
                bind_vars["end_time"] = time_range["end"].isoformat()
        
        where_clause = " AND ".join(filters) if filters else "true"
        
        # Use simple AQL search, if hybrid_search is to be used, it would be called here.
        # For this search_logs, it's a basic AQL with ArangoSearch view.
        aql = f"""
        FOR doc IN log_events_view
        SEARCH ANALYZER(doc.message IN TOKENS(@query, 'text_en'), 'text_en')
        FILTER {where_clause}
        SORT BM25(doc) DESC, doc.timestamp DESC
        LIMIT @limit
        RETURN doc
        """
        
        return await self.query_logs(aql, bind_vars)
    
    async def search_bm25_logs(
        self,
        text_query: str,
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """Full-text search using BM25 relevance."""
        # Use hybrid search module for advanced search
        if self.hybrid_search:
            results = await self.hybrid_search.search(
                query=text_query,
                search_type="bm25",
                collection="log_events",
                limit=limit,
                filters=filters
            )
            return results
        
        # Fallback to basic search if hybrid_search is not initialized/available
        return await self.search_logs(text_query, limit=limit)
    
    async def get_latest_response(
        self,
        script_name: str,
        execution_id: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """Get the latest structured response from a script."""
        bind_vars = {"script_name": script_name}
        
        if execution_id:
            bind_vars["execution_id"] = execution_id
            filter_clause = "doc.execution_id == @execution_id"
        else:
            filter_clause = "doc.script_name == @script_name"
        
        aql = f"""
        FOR doc IN log_events
        FILTER {filter_clause} AND doc.extra_data.response != null
        SORT doc.timestamp DESC
        LIMIT 1
        RETURN doc.extra_data.response
        """
        
        results = await self.query_logs(aql, bind_vars)
        return results[0] if results else None
    
    async def log_agent_learning(
        self,
        message: str,
        function_name: str,
        context: Optional[Dict[str, Any]] = None,
        confidence: float = 0.8
    ) -> None:
        """Record an agent learning or insight."""
        doc = {
            "timestamp": datetime.utcnow().isoformat(),
            "execution_id": self.current_execution_id or "manual",
            "learning": message,
            "function_name": function_name,
            "context": context or {},
            "confidence": confidence
        }
        
        try:
            await self.db.collection("agent_learnings").insert(doc)
            logger.info(f"Recorded agent learning: {message[:100]}...")
            
            # Also log to memory agent if available
            if self.memory_agent:
                await self.memory_agent.add_memory(
                    content=message,
                    memory_type="learning",
                    metadata={
                        "function": function_name,
                        "confidence": confidence,
                        **(context or {}) 
                    }
                )
                
        except Exception as e:
            logger.error(f"Failed to record agent learning: {e}")
    
    async def build_execution_graph(
        self,
        execution_id: str,
        include_relationships: bool = True
    ) -> Dict[str, Any]:
        """Build a graph representation of an execution."""
        # Get all logs for execution
        logs_aql = """
        FOR doc IN log_events
        FILTER doc.execution_id == @execution_id
        SORT doc.timestamp
        RETURN doc
        """
        
        logs = await self.query_logs(logs_aql, {"execution_id": execution_id})
        
        # Build graph structure
        graph = {
            "nodes": [],
            "edges": [],
            "metadata": {
                "execution_id": execution_id,
                "total_logs": len(logs)
            }
        }
        
        # Create nodes
        for i, log in enumerate(logs):
            node = {
                "id": log.get("_id", f"log_{i}"), 
                "label": f"{log['level']}: {log['message'][:50]}...",
                "timestamp": log["timestamp"],
                "level": log["level"],
                "function": log.get("function_name", "unknown")
            }
            graph["nodes"].append(node)
        
        # Extract relationships if requested
        if include_relationships and self.relationship_extractor:
            # Analyze log messages for relationships
            for i, log in enumerate(logs[:-1]):
                # Simple temporal relationship
                edge = {
                    "from": log.get("_id", f"log_{i}"),
                    "to": logs[i + 1].get("_id", f"log_{i + 1}"),
                    "type": "FOLLOWED_BY",
                    "weight": 1.0
                }
                graph["edges"].append(edge)
                
                # Extract semantic relationships using mock extractor
                rels_from_extractor = await self.relationship_extractor.extract_relationships(
                    log["message"],
                    logs[i + 1]["message"]
                )
                for rel in rels_from_extractor:
                    graph["edges"].append({
                        "from": log.get("_id", f"log_{i}"),
                        "to": logs[i + 1].get("_id", f"log_{i + 1}"),
                        "type": rel["type"],
                        "confidence": rel["confidence"]
                    })
        
        return graph
    
    async def prune_logs(
        self,
        older_than_days: Optional[int] = None,
        execution_ids: Optional[List[str]] = None,
        dry_run: bool = False
    ) -> Dict[str, int]:
        """Prune old logs based on criteria."""
        stats = {"examined": 0, "deleted": 0}
        
        # Build filter conditions
        filters = []
        bind_vars = {}
        
        if older_than_days:
            cutoff_date = datetime.utcnow() - timedelta(days=older_than_days)
            filters.append("doc.timestamp < @cutoff_date")
            bind_vars["cutoff_date"] = cutoff_date.isoformat()
        
        if execution_ids:
            filters.append("doc.execution_id IN @execution_ids")
            bind_vars["execution_ids"] = execution_ids
        
        if not filters:
            logger.warning("No pruning criteria specified")
            return stats
        
        where_clause = " AND ".join(filters)
        
        # Count matching documents
        count_aql = f"""
        FOR doc IN log_events
        FILTER {where_clause}
        COLLECT WITH COUNT INTO total
        RETURN total
        """
        
        count_result = await self.query_logs(count_aql, bind_vars)
        stats["examined"] = count_result[0] if count_result else 0
        
        if not dry_run and stats["examined"] > 0:
            # Delete matching documents
            delete_aql = f"""
            FOR doc IN log_events
            FILTER {where_clause}
            REMOVE doc IN log_events
            RETURN OLD
            """
            
            deleted = await self.query_logs(delete_aql, bind_vars)
            stats["deleted"] = len(deleted)
            
            logger.info(f"Pruned {stats['deleted']} logs")
        
        return stats
    
    async def get_execution_summary(self, execution_id: str) -> Dict[str, Any]:
        """Get comprehensive summary of a script execution."""
        # Get run info
        run_aql = """
        FOR doc IN script_runs
        FILTER doc.execution_id == @execution_id
        RETURN doc
        """
        
        run_info = await self.query_logs(run_aql, {"execution_id": execution_id})
        
        if not run_info:
            return {"error": "Execution not found"}
        
        # Get log statistics
        stats_aql = """
        FOR doc IN log_events
        FILTER doc.execution_id == @execution_id
        COLLECT level = doc.level WITH COUNT INTO count
        RETURN {level: level, count: count}
        """
        
        log_stats = await self.query_logs(stats_aql, {"execution_id": execution_id})
        
        # Get errors if any
        errors_aql = """
        FOR doc IN log_events
        FILTER doc.execution_id == @execution_id AND doc.level IN ["ERROR", "CRITICAL"]
        SORT doc.timestamp
        LIMIT 10
        RETURN {
            timestamp: doc.timestamp,
            message: doc.message,
            function: doc.function_name
        }
        """
        
        errors = await self.query_logs(errors_aql, {"execution_id": execution_id})
        
        # Get learnings
        learnings_aql = """
        FOR doc IN agent_learnings
        FILTER doc.execution_id == @execution_id
        RETURN {
            learning: doc.learning,
            confidence: doc.confidence,
            function: doc.function_name
        }
        """
        
        learnings = await self.query_logs(learnings_aql, {"execution_id": execution_id})
        
        summary = {
            "execution_id": execution_id,
            "run_info": run_info[0],
            "log_statistics": {stat["level"]: stat["count"] for stat in log_stats},
            "errors": errors,
            "learnings": learnings,
            "total_logs": sum(stat["count"] for stat in log_stats) if log_stats else 0
        }
        
        return summary


# Global instance getter
_manager_instance: Optional[AgentLogManager] = None


async def get_log_manager() -> AgentLogManager:
    """Get or create the global AgentLogManager instance."""
    global _manager_instance
    
    if _manager_instance is None:
        _manager_instance = AgentLogManager()
        
        db_config = {
            "url": os.getenv("ARANGO_URL", "http://localhost:8529"),
            "database": os.getenv("ARANGO_DATABASE", "script_logs"),
            "username": os.getenv("ARANGO_USERNAME", "root"),
            "password": os.getenv("ARANGO_PASSWORD", "openSesame")
        }
        
        await _manager_instance.initialize(db_config)
    
    return _manager_instance


async def working_usage():
    """Demonstrate AgentLogManager functionality."""
    logger.info("=== Testing AgentLogManager ===")
    
    manager = await get_log_manager()
    
    # Test 1: Script execution context
    # Note: The yield from script_execution is now the bound logger
    async with manager.script_execution("test_script", {"version": "1.0"}) as logger_ctx: 
        logger_ctx.info("Starting test operations")
        
        # Log some events
        logger_ctx.debug("Debug message")
        logger_ctx.info("Processing data")
        logger_ctx.warning("Resource usage high")
        
        # Log a learning
        await manager.log_agent_learning(
            "Discovered that batch size of 100 is optimal for this dataset",
            "process_data",
            {"batch_size": 100, "performance": "optimal"}
        )
        
        # Simulate some work
        await asyncio.sleep(1)
        
        logger_ctx.success("Operations completed")
    
    # Retrieve the execution_id from the manager after context exits
    # For a real scenario, you'd capture it when entering the context.
    # For this test, we'll get it from recent runs.
    recent_runs = await manager.query_logs(
        "FOR r IN script_runs SORT r.start_time DESC LIMIT 1 RETURN r"
    )
    exec_id = recent_runs[0]["execution_id"] if recent_runs else "unknown"


    # Test 2: Query logs
    logger.info(f"\nQuerying recent logs for execution: {exec_id}...")
    recent_logs = await manager.search_logs(
        "test",
        execution_id=exec_id,
        limit=5
    )
    logger.info(f"Found {len(recent_logs)} recent logs")
    
    # Test 3: Get execution summary
    logger.info(f"\nGetting execution summary for {exec_id}...")
    summary = await manager.get_execution_summary(exec_id)
    logger.info(f"Summary: {json.dumps(summary, indent=2)}")
    
    return True


async def debug_function():
    """Debug advanced features of AgentLogManager."""
    logger.info("=== Debug Mode: Advanced Features ===")
    
    manager = await get_log_manager()
    
    # Test 1: Complex AQL query
    logger.info("Test 1: Complex AQL query")
    aql = """
    FOR doc IN log_events
    FILTER doc.level IN ["ERROR", "CRITICAL"]
    COLLECT level = doc.level INTO logs
    RETURN {
        level: level,
        count: LENGTH(logs),
        recent: (
            FOR log IN logs
            SORT log.doc.timestamp DESC
            LIMIT 3
            RETURN log.doc.message
        )
    }
    """
    
    results = await manager.query_logs(aql)
    logger.info(f"Error summary: {results}")
    
    # Test 2: Build execution graph
    logger.info("\nTest 2: Building execution graph")
    
    # Create a test execution with related logs
    async with manager.script_execution("graph_test") as logger_ctx:
        logger_ctx.info("Step 1: Initialize")
        logger_ctx.info("Step 2: Load data")
        logger_ctx.error("Step 3: Connection failed")
        logger_ctx.info("Step 4: Retrying connection")
        logger_ctx.success("Step 5: Connection restored")
        
        await asyncio.sleep(1)

    recent_runs = await manager.query_logs(
        "FOR r IN script_runs SORT r.start_time DESC LIMIT 1 RETURN r"
    )
    exec_id_graph = recent_runs[0]["execution_id"] if recent_runs else "unknown"

    graph = await manager.build_execution_graph(exec_id_graph)
    logger.info(f"Graph nodes: {len(graph['nodes'])}, edges: {len(graph['edges'])}")
    
    # Test 3: Prune logs (dry run)
    logger.info("\nTest 3: Pruning old logs (dry run)")
    prune_stats = await manager.prune_logs(
        older_than_days=30,
        dry_run=True
    )
    logger.info(f"Would prune: {prune_stats}")
    
    return True


if __name__ == "__main__":
    import sys
    import os
    import socket 
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # Configure logger with ArangoDB sink
    from arango_log_sink import get_arango_sink
    
    # For standalone scripts, logger.remove() is used to ensure a clean slate,
    # preventing duplicate output if run multiple times in a session.
    # In a larger, long-running application, logging setup should be centralized
    # and might not involve calling logger.remove() indiscriminately.
    logger.remove() 
    logger.add(sys.stderr, level="INFO") # Add a console handler for general output
    
    sink = get_arango_sink()
    logger.add(sink.write, enqueue=True, level="DEBUG") # Add ArangoDB sink
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        success = False
        try:
            if mode == "debug":
                logger.info("Running in DEBUG mode...")
                success = await debug_function()
            else:
                logger.info("Running in WORKING mode...")
                success = await working_usage()
        except Exception as e:
            logger.error(f"Main execution failed: {e}")
            success = False
        finally:
            # Ensure sink is properly closed
            if sink:
                logger.info("Stopping ArangoLogSink...")
                await sink.stop()
                logger.info("ArangoLogSink stopped.")
        
        return success
    
    success = asyncio.run(main())
    exit(0 if success else 1)
```
