# Logger Agent Implementation Issues Report

**Date**: 2025-01-14  
**Generated by**: Claude Code Assistant  
**Purpose**: Document scripts that failed after 2 attempts for Gemini to fix

## Summary

This report documents the issues encountered when testing the Logger Agent scripts extracted from `gemini_logger_app.md`. While the utility and mock modules work correctly, the main Logger Agent modules fail due to missing dependencies.

## Working Scripts

The following scripts passed their `working_usage()` tests successfully:

1. **`src/utils/log_utils.py`** ✅
   - Fixed minor syntax error (missing quote in line 301)
   - All tests passed: log truncation, API logging, edge cases
   - Output saved to: `src/utils/tmp/responses/log_utils_results_*.json`

2. **`src/arangodb/core/search/hybrid_search.py`** ✅  
   - Mock implementation works correctly
   - All search tests passed (BM25, semantic, hybrid)
   - Output saved to: `src/arangodb/core/search/tmp/responses/hybrid_search_results_*.json`

3. **`src/arangodb/core/graph/relationship_extraction.py`** ✅
   - Mock implementation works correctly
   - Successfully extracts relationships (RETRY_OF, FIXED_BY, FOLLOWED_BY)
   - Output saved to: `src/arangodb/core/graph/tmp/responses/relationship_extraction_results_*.json`

4. **`src/arangodb/core/memory/memory_agent.py`** ✅
   - Mock implementation works correctly
   - Memory storage and retrieval tests passed
   - Output saved to: `src/arangodb/core/memory/tmp/responses/memory_agent_results_*.json`

## Failed Scripts

The following scripts failed after 2 attempts due to missing `aioarango` module. Each section includes the full script code for reference.

### 1. `src/arango_init.py` ❌

**Error**:
```
ModuleNotFoundError: No module named 'aioarango'
```

**Issue**: The script fails immediately on import because `aioarango` is not installed. This prevents the database initialization from running.

**Full Script Code**:
```python
#!/usr/bin/env python3
"""
arango_init.py - Initialize ArangoDB schema for Logger Agent

Creates database, collections, indexes, and ArangoSearch views.
Ensures idempotent execution for repeated runs.
"""

import asyncio
import os
from typing import Dict, Any, Optional
from pathlib import Path
from datetime import datetime

from aioarango import ArangoClient
from loguru import logger
from dotenv import load_dotenv
import uvloop

# Set uvloop as the event loop policy
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

# Load environment variables
load_dotenv()

# Configure logging
# For standalone scripts, removing default handler and adding a file/stderr handler is common.
# In a larger application, loguru configuration should be centralized.
logger.remove()
logger.add(
    "logs/arango_init_{time}.log",
    rotation="10 MB",
    retention=5,
    level="DEBUG"
)


async def create_database_and_collections():
    """Create database and collections with proper indexes."""
    
    # Connect to ArangoDB
    client = ArangoClient(hosts=os.getenv("ARANGO_URL", "http://localhost:8529"))
    
    # System database connection
    sys_db = await client.db(
        "_system",
        username=os.getenv("ARANGO_USERNAME", "root"),
        password=os.getenv("ARANGO_PASSWORD", "openSesame")
    )
    
    db_name = os.getenv("ARANGO_DATABASE", "script_logs")
    
    # Create database if not exists
    if not await sys_db.has_database(db_name):
        await sys_db.create_database(db_name)
        logger.info(f"Created database: {db_name}")
    
    # Connect to our database
    db = await client.db(
        db_name,
        username=os.getenv("ARANGO_USERNAME", "root"),
        password=os.getenv("ARANGO_PASSWORD", "openSesame")
    )
    
    # Create collections
    collections = {
        "log_events": {
            "schema": {
                "rule": {
                    "properties": {
                        "timestamp": {"type": "string"},
                        "level": {"type": "string"},
                        "message": {"type": "string"},
                        "execution_id": {"type": "string"},
                        "script_name": {"type": "string"},
                        "function_name": {"type": "string"},
                        "file_path": {"type": "string"},
                        "line_number": {"type": "integer"},
                        "extra_data": {"type": "object"},
                        "embeddings": {"type": "array"},
                        "tags": {"type": "array"}
                    },
                    "required": ["timestamp", "level", "message", "execution_id"]
                }
            }
        },
        "script_runs": {
            "schema": {
                "rule": {
                    "properties": {
                        "execution_id": {"type": "string"},
                        "script_name": {"type": "string"},
                        "start_time": {"type": "string"},
                        "end_time": {"type": "string"},
                        "status": {"type": "string"},
                        "metadata": {"type": "object"},
                        "error": {"type": "string"}
                    },
                    "required": ["execution_id", "script_name", "start_time"]
                }
            }
        },
        "agent_learnings": {
            "schema": {
                "rule": {
                    "properties": {
                        "timestamp": {"type": "string"},
                        "execution_id": {"type": "string"},
                        "learning": {"type": "string"},
                        "context": {"type": "object"},
                        "function_name": {"type": "string"},
                        "confidence": {"type": "number"}
                    },
                    "required": ["timestamp", "learning"]
                }
            }
        }
    }
    
    for coll_name, config in collections.items():
        if not await db.has_collection(coll_name):
            collection = await db.create_collection(
                coll_name,
                schema=config.get("schema")
            )
            logger.info(f"Created collection: {coll_name}")
        else:
            collection = db.collection(coll_name)
        
        # Create indexes
        if coll_name == "log_events":
            # Compound index for time-based queries
            await collection.add_persistent_index(
                fields=["execution_id", "timestamp"],
                unique=False,
                sparse=False
            )
            
            # Index for level-based filtering
            await collection.add_persistent_index(
                fields=["level", "timestamp"],
                unique=False,
                sparse=False
            )
            
            # Full-text index for message search
            await collection.add_fulltext_index(
                fields=["message"],
                min_length=3
            )
            
        elif coll_name == "script_runs":
            # Unique index on execution_id
            await collection.add_persistent_index(
                fields=["execution_id"],
                unique=True,
                sparse=False
            )
            
            # Index for script name queries
            await collection.add_persistent_index(
                fields=["script_name", "start_time"],
                unique=False,
                sparse=False
            )
    
    # Create ArangoSearch view
    view_name = "log_events_view"
    if not await db.has_view(view_name):
        await db.create_arangosearch_view(
            view_name,
            properties={
                "links": {
                    "log_events": {
                        "analyzers": ["text_en", "identity"],
                        "fields": {
                            "message": {
                                "analyzers": ["text_en"]
                            },
                            "level": {
                                "analyzers": ["identity"]
                            },
                            "script_name": {
                                "analyzers": ["identity"]
                            },
                            "tags": {
                                "analyzers": ["identity"]
                            }
                        },
                        "includeAllFields": False,
                        "storeValues": "id",
                        "trackListPositions": False
                    }
                }
            }
        )
        logger.info(f"Created ArangoSearch view: {view_name}")
    
    # Create graph for log relationships (optional)
    graph_name = "log_relationships"
    if not await db.has_graph(graph_name):
        await db.create_graph(
            graph_name,
            edge_definitions=[
                {
                    "edge_collection": "log_causality",
                    "from_vertex_collections": ["log_events"],
                    "to_vertex_collections": ["log_events", "agent_learnings"]
                }
            ]
        )
        logger.info(f"Created graph: {graph_name}")
    
    return db


async def working_usage():
    """Initialize database schema - stable working example."""
    logger.info("=== Initializing ArangoDB Schema ===")
    
    try:
        db = await create_database_and_collections()
        
        # Verify collections exist
        collections = await db.collections()
        logger.info(f"Available collections: {[c['name'] for c in collections]}")
        
        # Test write
        test_doc = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": "INFO",
            "message": "Database initialization test",
            "execution_id": "init_test_001",
            "script_name": "arango_init.py"
        }
        
        result = await db.collection("log_events").insert(test_doc)
        logger.success(f"Test document inserted: {result}")
        
        return True
        
    except Exception as e:
        logger.error(f"Database initialization failed: {e}")
        logger.exception("Full traceback:")
        return False


async def debug_function():
    """Debug function for testing schema modifications."""
    logger.info("=== Running Debug Mode ===")
    
    # Test experimental features
    client = ArangoClient(hosts=os.getenv("ARANGO_URL", "http://localhost:8529"))
    db = await client.db(
        os.getenv("ARANGO_DATABASE", "script_logs"),
        username=os.getenv("ARANGO_USERNAME", "root"),
        password=os.getenv("ARANGO_PASSWORD", "openSesame")
    )
    
    # Test APPROX_NEAR_COSINE availability
    test_vector = [0.1, 0.2, 0.3, 0.4, 0.5]
    query = """
    RETURN APPROX_NEAR_COSINE(@vector, @vector, 1)
    """
    
    try:
        cursor = await db.aql.execute(query, bind_vars={"vector": test_vector})
        result = await cursor.next()
        logger.success(f"APPROX_NEAR_COSINE test passed: {result}")
    except Exception as e:
        logger.error(f"APPROX_NEAR_COSINE not available: {e}")
        logger.warning("Ensure --query.enable-experimental flag is set")
    
    return True


if __name__ == "__main__":
    import sys
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        if mode == "debug":
            logger.info("Running in DEBUG mode...")
            success = await debug_function()
        else:
            logger.info("Running in WORKING mode...")
            success = await working_usage()
        
        return success
    
    success = asyncio.run(main())
    exit(0 if success else 1)
```

**Proposed Fix**: Add fallback imports and mock mode when dependencies are missing:
```python
# At the top of the file, after imports
try:
    from aioarango import ArangoClient
    AIOARANGO_AVAILABLE = True
except ImportError:
    AIOARANGO_AVAILABLE = False
    logger.warning("aioarango not available - running in mock mode")
    
    # Mock ArangoClient for testing
    class ArangoClient:
        def __init__(self, hosts):
            self.hosts = hosts
        async def db(self, name, username, password):
            return MockDatabase(name)
```

---

### 2. `src/arango_log_sink.py` ❌

**Error**:
```
ModuleNotFoundError: No module named 'aioarango'
```

**Issue**: The custom loguru sink cannot initialize because multiple dependencies are missing.

**Full Script Code**:
```python
#!/usr/bin/env python3
"""
arango_log_sink.py - Custom loguru sink for async ArangoDB writes

Implements non-blocking log ingestion with batching, buffering,
and automatic retry mechanisms.
"""

import asyncio
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from collections import deque
import aiofiles
import uvloop
import sys

from aioarango import ArangoClient
from loguru import logger
from tenacity import retry, stop_after_attempt, wait_exponential
import psutil

from utils.log_utils import log_safe_results

# Set uvloop as the event loop
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())


class ArangoLogSink:
    """Async sink for loguru that writes to ArangoDB with buffering."""
    
    def __init__(
        self,
        db_config: Dict[str, str],
        batch_size: int = 100,
        flush_interval: float = 2.0,
        buffer_dir: Path = Path("/tmp/logger_agent_buffer"),
        max_buffer_size_mb: int = 1000
    ):
        self.db_config = db_config
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.buffer_dir = buffer_dir
        self.max_buffer_size_mb = max_buffer_size_mb
        
        # Ensure buffer directory and a subdirectory for failed buffers exist
        self.buffer_dir.mkdir(parents=True, exist_ok=True)
        (self.buffer_dir / "_failed").mkdir(exist_ok=True) # Directory for problematic files
        
        # Log queue and batch
        self.log_queue: asyncio.Queue = asyncio.Queue(maxsize=10000)
        self.current_batch: deque = deque(maxlen=batch_size)
        
        # Database connection
        self.client = None
        self.db = None
        self.collection = None
        self.connected = False
        
        # Monitoring
        self.stats = {
            "total_logs": 0,
            "successful_writes": 0,
            "failed_writes": 0,
            "buffered_logs": 0,
            "last_error": None
        }
        
        # Background tasks
        self.consumer_task = None
        self.flush_task = None
        self.monitor_task = None
        
    async def connect(self) -> bool:
        """Establish connection to ArangoDB."""
        try:
            self.client = ArangoClient(hosts=self.db_config["url"])
            self.db = await self.client.db(
                self.db_config["database"],
                username=self.db_config["username"],
                password=self.db_config["password"]
            )
            self.collection = self.db.collection("log_events")
            
            # Test connection
            await self.db.version()
            self.connected = True
            logger.info("Connected to ArangoDB")
            return True
            
        except Exception as e:
            logger.error(f"Failed to connect to ArangoDB: {e}")
            self.connected = False
            self.stats["last_error"] = str(e)
            return False
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def write_batch_to_db(self, batch: List[Dict[str, Any]]) -> bool:
        """Write a batch of logs to ArangoDB with retry logic."""
        if not self.connected:
            if not await self.connect():
                raise Exception("Database connection failed") # Raise to trigger tenacity retry
        
        try:
            safe_batch = log_safe_results(batch) # Use log_safe_results here
            
            result = await self.collection.insert_many(safe_batch)
            self.stats["successful_writes"] += len(batch)
            return True
            
        except Exception as e:
            logger.error(f"Failed to write batch: {e}")
            self.stats["failed_writes"] += len(batch)
            self.stats["last_error"] = str(e)
            self.connected = False  # Mark as disconnected for next attempt
            raise # Re-raise to trigger tenacity retry
    
    async def buffer_to_disk(self, logs: List[Dict[str, Any]]) -> None:
        """Buffer logs to disk when database is unavailable."""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S_%f")
        buffer_file = self.buffer_dir / f"buffer_{timestamp}.jsonl"
        
        try:
            async with aiofiles.open(buffer_file, 'w') as f:
                for log in logs:
                    await f.write(json.dumps(log) + '\n')
            
            self.stats["buffered_logs"] += len(logs)
            logger.warning(f"Buffered {len(logs)} logs to {buffer_file}")
            
            # Check buffer size and clean if needed
            await self.check_buffer_size()
        except Exception as e:
            logger.critical(f"Failed to write logs to disk buffer {buffer_file}: {e}")
            # At this point, logs are effectively lost if disk buffering fails.
            # This should ideally trigger an external alert.
    
    async def check_buffer_size(self) -> None:
        """Monitor buffer directory size and clean old files if needed."""
        if not self.buffer_dir.exists():
            return
            
        total_size = sum(f.stat().st_size for f in self.buffer_dir.glob("*.jsonl"))
        total_size_mb = total_size / (1024 * 1024)
        
        if total_size_mb > self.max_buffer_size_mb:
            # Remove oldest files
            files = sorted(self.buffer_dir.glob("*.jsonl"), key=lambda f: f.stat().st_mtime)
            
            while total_size_mb > self.max_buffer_size_mb * 0.8 and files:
                oldest = files.pop(0)
                try:
                    size = oldest.stat().st_size
                    oldest.unlink()
                    total_size_mb -= size / (1024 * 1024)
                    logger.warning(f"Removed old buffer file: {oldest.name} to stay within buffer limits.")
                except OSError as e:
                    logger.error(f"Failed to delete old buffer file {oldest.name}: {e}")
    
    async def process_buffered_logs(self) -> None:
        """Process buffered logs when connection is restored."""
        if not self.connected or not self.buffer_dir.exists():
            return
        
        buffer_files = sorted(self.buffer_dir.glob("*.jsonl"))
        failed_buffer_dir = self.buffer_dir / "_failed"
        failed_buffer_dir.mkdir(exist_ok=True) # Ensure it exists
        
        for buffer_file in buffer_files:
            try:
                logs = []
                async with aiofiles.open(buffer_file, 'r') as f:
                    async for line in f:
                        logs.append(json.loads(line.strip()))
                
                if logs:
                    # Process in batches
                    for i in range(0, len(logs), self.batch_size):
                        batch = logs[i:i + self.batch_size]
                        try:
                            await self.write_batch_to_db(batch)
                        except Exception as inner_e:
                            logger.error(f"Persistent failure to write batch from buffered file {buffer_file.name}: {inner_e}. Re-buffering this part of the file.")
                            # If write_batch_to_db fails even after its retries, re-buffer the failed batch
                            await self.buffer_to_disk(batch) 
                            # Move the original problematic file to _failed and stop processing this file
                            buffer_file.rename(failed_buffer_dir / buffer_file.name)
                            logger.warning(f"Moved problematic buffer file to {failed_buffer_dir.name}: {buffer_file.name}")
                            break # Stop processing current file, move to next
                    else: # This 'else' runs if the for loop completed without a 'break'
                        # If all batches from this file were successfully written, remove it
                        buffer_file.unlink()
                        logger.info(f"Processed and removed buffered file: {buffer_file.name}")
                else:
                    # If file is empty, remove it
                    buffer_file.unlink()
                    logger.info(f"Removed empty buffered file: {buffer_file.name}")
                
            except Exception as e:
                logger.error(f"Failed to process or parse buffer file {buffer_file.name}: {e}")
                # Move problematic file to a 'failed' sub-directory for manual inspection
                try:
                    buffer_file.rename(failed_buffer_dir / buffer_file.name)
                    logger.warning(f"Moved problematic buffer file to {failed_buffer_dir.name}: {buffer_file.name}")
                except Exception as move_e:
                    logger.critical(f"Could not move problematic file {buffer_file.name} to _failed directory: {move_e}")
                # Continue to next file, don't break
                continue
    
    async def log_consumer(self) -> None:
        """Consume logs from queue and batch them."""
        while True:
            try:
                # Get log from queue with timeout
                log_data = await asyncio.wait_for(
                    self.log_queue.get(),
                    timeout=self.flush_interval
                )
                
                self.current_batch.append(log_data)
                self.stats["total_logs"] += 1
                
                # Flush if batch is full
                if len(self.current_batch) >= self.batch_size:
                    await self.flush_batch()
                
            except asyncio.TimeoutError:
                # Flush on timeout
                if self.current_batch:
                    await self.flush_batch()
            except Exception as e:
                logger.error(f"Error in log consumer: {e}")
    
    async def flush_batch(self) -> None:
        """Flush current batch to database or disk buffer."""
        if not self.current_batch:
            return
        
        batch = list(self.current_batch)
        self.current_batch.clear()
        
        try:
            await self.write_batch_to_db(batch)
            
            # Try to process buffered logs after successful write
            await self.process_buffered_logs()
            
        except Exception: # Catch any exception from write_batch_to_db, which already logs failures
            # Buffer to disk on failure
            await self.buffer_to_disk(batch)
    
    async def periodic_flush(self) -> None:
        """Periodically flush logs."""
        while True:
            await asyncio.sleep(self.flush_interval)
            await self.flush_batch()
    
    async def monitor_performance(self) -> None:
        """Monitor sink performance and alert on issues."""
        alert_threshold = int(os.getenv("ALERT_LOG_FAILURE_THRESHOLD", "10"))
        monitoring_interval = int(os.getenv("MONITORING_INTERVAL", "60"))
        
        while True:
            await asyncio.sleep(monitoring_interval)
            
            # Calculate failure rate
            total = self.stats["successful_writes"] + self.stats["failed_writes"]
            if total > 0:
                failure_rate = self.stats["failed_writes"] / total * 100
                
                if failure_rate > alert_threshold:
                    logger.critical(
                        f"High log failure rate: {failure_rate:.1f}% "
                        f"({self.stats['failed_writes']}/{total} failed)"
                    )
            
            # Check memory usage
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 80:
                logger.warning(f"High memory usage: {memory_percent}%")
            
            # Log stats
            logger.info(f"Log sink stats: {self.stats}")
    
    async def start(self) -> None:
        """Start background tasks."""
        await self.connect()
        
        self.consumer_task = asyncio.create_task(self.log_consumer())
        self.flush_task = asyncio.create_task(self.periodic_flush())
        
        if os.getenv("ENABLE_MONITORING", "true").lower() == "true":
            self.monitor_task = asyncio.create_task(self.monitor_performance())
    
    async def stop(self) -> None:
        """Stop background tasks and flush remaining logs."""
        # Cancel tasks
        for task in [self.consumer_task, self.flush_task, self.monitor_task]:
            if task:
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass
        
        # Final flush
        await self.flush_batch()
        
        # Close database connection
        if self.client:
            await self.client.close()
    
    def write(self, message: Dict[str, Any]) -> None:
        """Synchronous write method for loguru compatibility."""
        # Parse loguru message
        record = message.record
        
        # Extract file name without path, and function from record
        script_name = Path(record["file"].name).stem
        function_name = record["function"]
        
        log_data = {
            "timestamp": record["time"].isoformat(),
            "level": record["level"].name,
            "message": record["message"],
            "execution_id": record["extra"].get("execution_id", "unknown"),
            "script_name": record["extra"].get("script_name", script_name),
            "function_name": function_name,
            "file_path": record["file"].path,
            "line_number": record["line"],
            "extra_data": record["extra"],
            "tags": record["extra"].get("tags", [])
        }
        
        # Add to queue (non-blocking)
        try:
            self.log_queue.put_nowait(log_data)
        except asyncio.QueueFull:
            # Queue is full, log to stderr as fallback
            # IMPORTANT: Using logger.bind(skip_sink=True) for fallback to avoid infinite loop
            logger.bind(skip_sink=True).error(
                f"Log queue full, dropping log: {log_data['message'][:100]}... "
                f"(Total buffered: {self.stats['buffered_logs']})"
            )


# Global sink instance
_sink_instance: Optional[ArangoLogSink] = None


def get_arango_sink() -> ArangoLogSink:
    """
    Get or create the global ArangoDB sink instance.
    
    NOTE: This function implicitly starts the sink's background tasks
    (consumer, flusher, monitor) the first time it is called by creating
    an `asyncio.Task`. In a larger, more structured application, it might be
    preferable to explicitly manage the sink's lifecycle (e.g., calling
    `sink.start()` and `sink.stop()` during application boot/shutdown).
    For standalone scripts using `asyncio.run()`, this pattern is acceptable.
    """
    global _sink_instance
    
    if _sink_instance is None:
        db_config = {
            "url": os.getenv("ARANGO_URL", "http://localhost:8529"),
            "database": os.getenv("ARANGO_DATABASE", "script_logs"),
            "username": os.getenv("ARANGO_USERNAME", "root"),
            "password": os.getenv("ARANGO_PASSWORD", "openSesame")
        }
        
        _sink_instance = ArangoLogSink(
            db_config=db_config,
            batch_size=int(os.getenv("LOG_BATCH_SIZE", "200")),
            flush_interval=float(os.getenv("LOG_FLUSH_INTERVAL", "2")),
            buffer_dir=Path(os.getenv("LOG_BUFFER_DIR", "/tmp/logger_agent_buffer")),
            max_buffer_size_mb=int(os.getenv("LOG_MAX_BUFFER_SIZE_MB", "1000"))
        )
        
        # Start sink in background as an asyncio task.
        # This task will run as long as the event loop is active.
        asyncio.create_task(_sink_instance.start())
    
    return _sink_instance


async def working_usage():
    """Test the ArangoDB sink with sample logs."""
    logger.info("=== Testing ArangoDB Sink ===")
    
    # Configure logger with our sink
    sink = get_arango_sink()
    logger.add(sink.write, enqueue=True)
    
    # Generate test logs
    execution_id = f"test_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
    
    logger.bind(execution_id=execution_id, tags=["test", "sink"]).info("Test log 1")
    logger.bind(execution_id=execution_id, tags=["test"]).warning("Test warning")
    logger.bind(execution_id=execution_id).error("Test error")
    
    # Wait for logs to be written
    await asyncio.sleep(3)
    
    # Check stats
    logger.info(f"Sink stats: {sink.stats}")
    
    return True


async def debug_function():
    """Debug sink behavior under various conditions."""
    logger.info("=== Debug Mode: Testing Edge Cases ===")
    
    sink = get_arango_sink()
    logger.add(sink.write, enqueue=True)
    
    # Test 1: High volume logging
    logger.info("Test 1: High volume")
    for i in range(1000):
        logger.bind(test_id=f"volume_{i}").debug(f"High volume test {i}")
    
    await asyncio.sleep(5)
    logger.info(f"After high volume: {sink.stats}")
    
    # Test 2: Large messages
    logger.info("Test 2: Large messages")
    large_data = {"data": "x" * 10000, "array": list(range(1000))}
    logger.bind(extra_data=large_data).info("Large message test")
    
    await asyncio.sleep(2)
    
    # Test 3: Connection failure simulation
    logger.info("Test 3: Simulating connection failure (logs will buffer to disk)")
    
    # Temporarily disable connection
    sink.connected = False
    
    for i in range(10):
        logger.bind(test="failover").error(f"Failover test {i}")
    
    await asyncio.sleep(3) # Give time for logs to be put in queue and attempt write
    logger.info(f"After failover (buffered logs might not show up immediately in DB): {sink.stats}")
    
    # Re-enable connection and process buffered files
    logger.info("Re-enabling connection to process buffered logs...")
    await sink.connect() # Attempt to reconnect
    await asyncio.sleep(5) # Give time for processing buffered logs
    
    logger.info(f"After reconnect and processing: {sink.stats}")

    # Check buffer files
    buffer_files = list(sink.buffer_dir.glob("*.jsonl"))
    failed_buffer_files = list((sink.buffer_dir / "_failed").glob("*.jsonl"))
    logger.info(f"Remaining buffer files: {len(buffer_files)}")
    logger.info(f"Failed buffer files (quarantined): {len(failed_buffer_files)}")
    
    return True


if __name__ == "__main__":
    import sys
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # Configure root logger with a basic console handler as a fallback.
    # For standalone scripts, logger.remove() is used to ensure a clean slate,
    # preventing duplicate output if run multiple times in a session.
    # In a larger, long-running application, logging setup should be centralized
    # and might not involve calling logger.remove() indiscriminately.
    logger.remove() 
    logger.add(sys.stderr, level="INFO")

    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        success = False
        try:
            if mode == "debug":
                logger.info("Running in DEBUG mode...")
                success = await debug_function()
            else:
                logger.info("Running in WORKING mode...")
                success = await working_usage()
            
        finally:
            # Cleanup
            sink = get_arango_sink()
            if sink:
                logger.info("Stopping ArangoLogSink...")
                await sink.stop()
                logger.info("ArangoLogSink stopped.")
        
        return success
    
    success = asyncio.run(main())
    exit(0 if success else 1)
```

**Proposed Fix**: Implement graceful degradation with optional dependencies:
```python
# Check for optional dependencies
try:
    import aiofiles
    AIOFILES_AVAILABLE = True
except ImportError:
    AIOFILES_AVAILABLE = False
    logger.warning("aiofiles not available - using synchronous file I/O")

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    logger.warning("psutil not available - memory monitoring disabled")
```

---

### 3. `src/agent_log_manager.py` ❌

**Error**:
```
ModuleNotFoundError: No module named 'aioarango'
```

**Issue**: The main log manager cannot initialize without database connection capabilities.

**Full Script Code**:
```python
#!/usr/bin/env python3
"""
agent_log_manager.py - Unified API for agent logging and introspection

Provides a singleton interface for agents to log, query, and analyze
their execution history using ArangoDB backend.
"""

import asyncio
import json
import uuid
import os 
import socket 
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from contextlib import asynccontextmanager

from aioarango import ArangoClient
from loguru import logger
# numpy is imported but not used directly in the provided snippet's logic, keeping it for compatibility if other parts of the project use it.
import numpy as np 
from tenacity import retry, stop_after_attempt, wait_exponential

# Import from existing modules (MOCK implementations)
from arangodb.core.search.hybrid_search import HybridSearch
from arangodb.core.graph.relationship_extraction import RelationshipExtractor
from arangodb.core.memory.memory_agent import MemoryAgent
from utils.log_utils import truncate_large_value 


class AgentLogManager:
    """Singleton manager for agent logging operations."""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.db = None
            self.client = None
            self.execution_context = {}
            self.current_execution_id = None
            
            # Integration modules
            self.hybrid_search = None
            self.relationship_extractor = None
            self.memory_agent = None
            
            self._initialized = True
    
    async def initialize(self, db_config: Dict[str, str]) -> None:
        """Initialize database connection and integration modules."""
        try:
            self.client = ArangoClient(hosts=db_config["url"])
            self.db = await self.client.db(
                db_config["database"],
                username=db_config["username"],
                password=db_config["password"]
            )
            
            # Initialize integration modules
            self.hybrid_search = HybridSearch(self.db)
            self.relationship_extractor = RelationshipExtractor(self.db)
            self.memory_agent = MemoryAgent(self.db)
            
            logger.info("AgentLogManager initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize AgentLogManager: {e}")
            raise
    
    def generate_execution_id(self, script_name: str) -> str:
        """Generate unique execution ID."""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        return f"{script_name}_{timestamp}_{unique_id}"
    
    @asynccontextmanager
    async def script_execution(self, script_name: str, metadata: Optional[Dict] = None):
        """
        Context manager for script execution tracking.
        Yields a Loguru logger instance with `execution_id` and `script_name` bound.
        """
        execution_id = self.generate_execution_id(script_name)
        self.current_execution_id = execution_id
        
        # Start script run
        await self.start_run(script_name, execution_id, metadata)
        
        try:
            # Bind execution context to logger (using an existing loguru feature)
            logger_with_context = logger.bind(
                execution_id=execution_id,
                script_name=script_name
            )
            
            yield logger_with_context
            
            # Mark as successful
            await self.end_run(execution_id, "success")
            
        except Exception as e:
            # Mark as failed
            logger.error(f"Script {script_name} failed with error: {e}", 
                         execution_id=execution_id, script_name=script_name)
            await self.end_run(execution_id, "failed", str(e))
            raise
        
        finally:
            self.current_execution_id = None
    
    async def start_run(
        self,
        script_name: str,
        execution_id: str,
        metadata: Optional[Dict] = None
    ) -> None:
        """Record script execution start."""
        doc = {
            "execution_id": execution_id,
            "script_name": script_name,
            "start_time": datetime.utcnow().isoformat(),
            "status": "running",
            "metadata": metadata or {},
            "pid": os.getpid(),
            "hostname": socket.gethostname()
        }
        
        try:
            await self.db.collection("script_runs").insert(doc)
            logger.info(f"Started script run: {execution_id}")
        except Exception as e:
            logger.error(f"Failed to record script start: {e}")
    
    async def end_run(
        self,
        execution_id: str,
        status: str = "success",
        error: Optional[str] = None
    ) -> None:
        """Record script execution end."""
        update_doc = {
            "end_time": datetime.utcnow().isoformat(),
            "status": status,
            "duration_seconds": None  # Will calculate from start_time
        }
        
        if error:
            update_doc["error"] = truncate_large_value(error, max_str_len=1000)
        
        try:
            # Get start time to calculate duration
            run_doc = await self.db.collection("script_runs").get({"execution_id": execution_id})
            if run_doc:
                start_time = datetime.fromisoformat(run_doc["start_time"])
                end_time = datetime.fromisoformat(update_doc["end_time"])
                update_doc["duration_seconds"] = (end_time - start_time).total_seconds()
            
            await self.db.collection("script_runs").update_match(
                {"execution_id": execution_id},
                update_doc
            )
            logger.info(f"Ended script run: {execution_id} ({status})")
            
        except Exception as e:
            logger.error(f"Failed to record script end: {e}")
    
    async def query_logs(
        self,
        aql_query: str,
        bind_vars: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Execute custom AQL query on logs."""
        try:
            cursor = await self.db.aql.execute(
                aql_query,
                bind_vars=bind_vars or {},
                batch_size=100
            )
            
            results = []
            async for doc in cursor:
                results.append(doc)
            
            return results
            
        except Exception as e:
            logger.error(f"AQL query failed: {e}")
            raise
    
    async def search_logs(
        self,
        query: str,
        execution_id: Optional[str] = None,
        level: Optional[str] = None,
        time_range: Optional[Dict[str, datetime]] = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Search logs with multiple filters."""
        # Build AQL query
        filters = []
        bind_vars = {"query": query, "limit": limit}
        
        if execution_id:
            filters.append("doc.execution_id == @execution_id")
            bind_vars["execution_id"] = execution_id
        
        if level:
            filters.append("doc.level == @level")
            bind_vars["level"] = level
        
        if time_range:
            if "start" in time_range:
                filters.append("doc.timestamp >= @start_time")
                bind_vars["start_time"] = time_range["start"].isoformat()
            if "end" in time_range:
                filters.append("doc.timestamp <= @end_time")
                bind_vars["end_time"] = time_range["end"].isoformat()
        
        where_clause = " AND ".join(filters) if filters else "true"
        
        # Use simple AQL search, if hybrid_search is to be used, it would be called here.
        # For this search_logs, it's a basic AQL with ArangoSearch view.
        aql = f"""
        FOR doc IN log_events_view
        SEARCH ANALYZER(doc.message IN TOKENS(@query, 'text_en'), 'text_en')
        FILTER {where_clause}
        SORT BM25(doc) DESC, doc.timestamp DESC
        LIMIT @limit
        RETURN doc
        """
        
        return await self.query_logs(aql, bind_vars)
    
    async def search_bm25_logs(
        self,
        text_query: str,
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """Full-text search using BM25 relevance."""
        # Use hybrid search module for advanced search
        if self.hybrid_search:
            results = await self.hybrid_search.search(
                query=text_query,
                search_type="bm25",
                collection="log_events",
                limit=limit,
                filters=filters
            )
            return results
        
        # Fallback to basic search if hybrid_search is not initialized/available
        return await self.search_logs(text_query, limit=limit)
    
    async def get_latest_response(
        self,
        script_name: str,
        execution_id: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """Get the latest structured response from a script."""
        bind_vars = {"script_name": script_name}
        
        if execution_id:
            bind_vars["execution_id"] = execution_id
            filter_clause = "doc.execution_id == @execution_id"
        else:
            filter_clause = "doc.script_name == @script_name"
        
        aql = f"""
        FOR doc IN log_events
        FILTER {filter_clause} AND doc.extra_data.response != null
        SORT doc.timestamp DESC
        LIMIT 1
        RETURN doc.extra_data.response
        """
        
        results = await self.query_logs(aql, bind_vars)
        return results[0] if results else None
    
    async def log_agent_learning(
        self,
        message: str,
        function_name: str,
        context: Optional[Dict[str, Any]] = None,
        confidence: float = 0.8
    ) -> None:
        """Record an agent learning or insight."""
        doc = {
            "timestamp": datetime.utcnow().isoformat(),
            "execution_id": self.current_execution_id or "manual",
            "learning": message,
            "function_name": function_name,
            "context": context or {},
            "confidence": confidence
        }
        
        try:
            await self.db.collection("agent_learnings").insert(doc)
            logger.info(f"Recorded agent learning: {message[:100]}...")
            
            # Also log to memory agent if available
            if self.memory_agent:
                await self.memory_agent.add_memory(
                    content=message,
                    memory_type="learning",
                    metadata={
                        "function": function_name,
                        "confidence": confidence,
                        **(context or {}) 
                    }
                )
                
        except Exception as e:
            logger.error(f"Failed to record agent learning: {e}")
    
    async def build_execution_graph(
        self,
        execution_id: str,
        include_relationships: bool = True
    ) -> Dict[str, Any]:
        """Build a graph representation of an execution."""
        # Get all logs for execution
        logs_aql = """
        FOR doc IN log_events
        FILTER doc.execution_id == @execution_id
        SORT doc.timestamp
        RETURN doc
        """
        
        logs = await self.query_logs(logs_aql, {"execution_id": execution_id})
        
        # Build graph structure
        graph = {
            "nodes": [],
            "edges": [],
            "metadata": {
                "execution_id": execution_id,
                "total_logs": len(logs)
            }
        }
        
        # Create nodes
        for i, log in enumerate(logs):
            node = {
                "id": log.get("_id", f"log_{i}"), 
                "label": f"{log['level']}: {log['message'][:50]}...",
                "timestamp": log["timestamp"],
                "level": log["level"],
                "function": log.get("function_name", "unknown")
            }
            graph["nodes"].append(node)
        
        # Extract relationships if requested
        if include_relationships and self.relationship_extractor:
            # Analyze log messages for relationships
            for i, log in enumerate(logs[:-1]):
                # Simple temporal relationship
                edge = {
                    "from": log.get("_id", f"log_{i}"),
                    "to": logs[i + 1].get("_id", f"log_{i + 1}"),
                    "type": "FOLLOWED_BY",
                    "weight": 1.0
                }
                graph["edges"].append(edge)
                
                # Extract semantic relationships using mock extractor
                rels_from_extractor = await self.relationship_extractor.extract_relationships(
                    log["message"],
                    logs[i + 1]["message"]
                )
                for rel in rels_from_extractor:
                    graph["edges"].append({
                        "from": log.get("_id", f"log_{i}"),
                        "to": logs[i + 1].get("_id", f"log_{i + 1}"),
                        "type": rel["type"],
                        "confidence": rel["confidence"]
                    })
        
        return graph
    
    async def prune_logs(
        self,
        older_than_days: Optional[int] = None,
        execution_ids: Optional[List[str]] = None,
        dry_run: bool = False
    ) -> Dict[str, int]:
        """Prune old logs based on criteria."""
        stats = {"examined": 0, "deleted": 0}
        
        # Build filter conditions
        filters = []
        bind_vars = {}
        
        if older_than_days:
            cutoff_date = datetime.utcnow() - timedelta(days=older_than_days)
            filters.append("doc.timestamp < @cutoff_date")
            bind_vars["cutoff_date"] = cutoff_date.isoformat()
        
        if execution_ids:
            filters.append("doc.execution_id IN @execution_ids")
            bind_vars["execution_ids"] = execution_ids
        
        if not filters:
            logger.warning("No pruning criteria specified")
            return stats
        
        where_clause = " AND ".join(filters)
        
        # Count matching documents
        count_aql = f"""
        FOR doc IN log_events
        FILTER {where_clause}
        COLLECT WITH COUNT INTO total
        RETURN total
        """
        
        count_result = await self.query_logs(count_aql, bind_vars)
        stats["examined"] = count_result[0] if count_result else 0
        
        if not dry_run and stats["examined"] > 0:
            # Delete matching documents
            delete_aql = f"""
            FOR doc IN log_events
            FILTER {where_clause}
            REMOVE doc IN log_events
            RETURN OLD
            """
            
            deleted = await self.query_logs(delete_aql, bind_vars)
            stats["deleted"] = len(deleted)
            
            logger.info(f"Pruned {stats['deleted']} logs")
        
        return stats
    
    async def get_execution_summary(self, execution_id: str) -> Dict[str, Any]:
        """Get comprehensive summary of a script execution."""
        # Get run info
        run_aql = """
        FOR doc IN script_runs
        FILTER doc.execution_id == @execution_id
        RETURN doc
        """
        
        run_info = await self.query_logs(run_aql, {"execution_id": execution_id})
        
        if not run_info:
            return {"error": "Execution not found"}
        
        # Get log statistics
        stats_aql = """
        FOR doc IN log_events
        FILTER doc.execution_id == @execution_id
        COLLECT level = doc.level WITH COUNT INTO count
        RETURN {level: level, count: count}
        """
        
        log_stats = await self.query_logs(stats_aql, {"execution_id": execution_id})
        
        # Get errors if any
        errors_aql = """
        FOR doc IN log_events
        FILTER doc.execution_id == @execution_id AND doc.level IN ["ERROR", "CRITICAL"]
        SORT doc.timestamp
        LIMIT 10
        RETURN {
            timestamp: doc.timestamp,
            message: doc.message,
            function: doc.function_name
        }
        """
        
        errors = await self.query_logs(errors_aql, {"execution_id": execution_id})
        
        # Get learnings
        learnings_aql = """
        FOR doc IN agent_learnings
        FILTER doc.execution_id == @execution_id
        RETURN {
            learning: doc.learning,
            confidence: doc.confidence,
            function: doc.function_name
        }
        """
        
        learnings = await self.query_logs(learnings_aql, {"execution_id": execution_id})
        
        summary = {
            "execution_id": execution_id,
            "run_info": run_info[0],
            "log_statistics": {stat["level"]: stat["count"] for stat in log_stats},
            "errors": errors,
            "learnings": learnings,
            "total_logs": sum(stat["count"] for stat in log_stats) if log_stats else 0
        }
        
        return summary


# Global instance getter
_manager_instance: Optional[AgentLogManager] = None


async def get_log_manager() -> AgentLogManager:
    """Get or create the global AgentLogManager instance."""
    global _manager_instance
    
    if _manager_instance is None:
        _manager_instance = AgentLogManager()
        
        db_config = {
            "url": os.getenv("ARANGO_URL", "http://localhost:8529"),
            "database": os.getenv("ARANGO_DATABASE", "script_logs"),
            "username": os.getenv("ARANGO_USERNAME", "root"),
            "password": os.getenv("ARANGO_PASSWORD", "openSesame")
        }
        
        await _manager_instance.initialize(db_config)
    
    return _manager_instance


async def working_usage():
    """Demonstrate AgentLogManager functionality."""
    logger.info("=== Testing AgentLogManager ===")
    
    manager = await get_log_manager()
    
    # Test 1: Script execution context
    # Note: The yield from script_execution is now the bound logger
    async with manager.script_execution("test_script", {"version": "1.0"}) as logger_ctx: 
        logger_ctx.info("Starting test operations")
        
        # Log some events
        logger_ctx.debug("Debug message")
        logger_ctx.info("Processing data")
        logger_ctx.warning("Resource usage high")
        
        # Log a learning
        await manager.log_agent_learning(
            "Discovered that batch size of 100 is optimal for this dataset",
            "process_data",
            {"batch_size": 100, "performance": "optimal"}
        )
        
        # Simulate some work
        await asyncio.sleep(1)
        
        logger_ctx.success("Operations completed")
    
    # Retrieve the execution_id from the manager after context exits
    # For a real scenario, you'd capture it when entering the context.
    # For this test, we'll get it from recent runs.
    recent_runs = await manager.query_logs(
        "FOR r IN script_runs SORT r.start_time DESC LIMIT 1 RETURN r"
    )
    exec_id = recent_runs[0]["execution_id"] if recent_runs else "unknown"


    # Test 2: Query logs
    logger.info(f"\nQuerying recent logs for execution: {exec_id}...")
    recent_logs = await manager.search_logs(
        "test",
        execution_id=exec_id,
        limit=5
    )
    logger.info(f"Found {len(recent_logs)} recent logs")
    
    # Test 3: Get execution summary
    logger.info(f"\nGetting execution summary for {exec_id}...")
    summary = await manager.get_execution_summary(exec_id)
    logger.info(f"Summary: {json.dumps(summary, indent=2)}")
    
    return True


async def debug_function():
    """Debug advanced features of AgentLogManager."""
    logger.info("=== Debug Mode: Advanced Features ===")
    
    manager = await get_log_manager()
    
    # Test 1: Complex AQL query
    logger.info("Test 1: Complex AQL query")
    aql = """
    FOR doc IN log_events
    FILTER doc.level IN ["ERROR", "CRITICAL"]
    COLLECT level = doc.level INTO logs
    RETURN {
        level: level,
        count: LENGTH(logs),
        recent: (
            FOR log IN logs
            SORT log.doc.timestamp DESC
            LIMIT 3
            RETURN log.doc.message
        )
    }
    """
    
    results = await manager.query_logs(aql)
    logger.info(f"Error summary: {results}")
    
    # Test 2: Build execution graph
    logger.info("\nTest 2: Building execution graph")
    
    # Create a test execution with related logs
    async with manager.script_execution("graph_test") as logger_ctx:
        logger_ctx.info("Step 1: Initialize")
        logger_ctx.info("Step 2: Load data")
        logger_ctx.error("Step 3: Connection failed")
        logger_ctx.info("Step 4: Retrying connection")
        logger_ctx.success("Step 5: Connection restored")
        
        await asyncio.sleep(1)

    recent_runs = await manager.query_logs(
        "FOR r IN script_runs SORT r.start_time DESC LIMIT 1 RETURN r"
    )
    exec_id_graph = recent_runs[0]["execution_id"] if recent_runs else "unknown"

    graph = await manager.build_execution_graph(exec_id_graph)
    logger.info(f"Graph nodes: {len(graph['nodes'])}, edges: {len(graph['edges'])}")
    
    # Test 3: Prune logs (dry run)
    logger.info("\nTest 3: Pruning old logs (dry run)")
    prune_stats = await manager.prune_logs(
        older_than_days=30,
        dry_run=True
    )
    logger.info(f"Would prune: {prune_stats}")
    
    return True


if __name__ == "__main__":
    import sys
    import os
    import socket 
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # Configure logger with ArangoDB sink
    from arango_log_sink import get_arango_sink
    
    # For standalone scripts, logger.remove() is used to ensure a clean slate,
    # preventing duplicate output if run multiple times in a session.
    # In a larger, long-running application, logging setup should be centralized
    # and might not involve calling logger.remove() indiscriminately.
    logger.remove() 
    logger.add(sys.stderr, level="INFO") # Add a console handler for general output
    
    sink = get_arango_sink()
    logger.add(sink.write, enqueue=True, level="DEBUG") # Add ArangoDB sink
    
    mode = sys.argv[1] if len(sys.argv) > 1 else "working"
    
    async def main():
        success = False
        try:
            if mode == "debug":
                logger.info("Running in DEBUG mode...")
                success = await debug_function()
            else:
                logger.info("Running in WORKING mode...")
                success = await working_usage()
        except Exception as e:
            logger.error(f"Main execution failed: {e}")
            success = False
        finally:
            # Ensure sink is properly closed
            if sink:
                logger.info("Stopping ArangoLogSink...")
                await sink.stop()
                logger.info("ArangoLogSink stopped.")
        
        return success
    
    success = asyncio.run(main())
    exit(0 if success else 1)
```

**Proposed Fix**: Create a mock mode that works without database dependencies:
```python
# Add at top of file
MOCK_MODE = False

try:
    from aioarango import ArangoClient
except ImportError:
    MOCK_MODE = True
    logger.warning("Running in MOCK mode - no database operations will be performed")
    
    # Mock implementations
    class MockDatabase:
        async def collection(self, name):
            return MockCollection(name)
        
        async def aql(self):
            return MockAQL()
    
    class MockCollection:
        def __init__(self, name):
            self.name = name
        
        async def insert(self, doc):
            return {"_id": f"mock/{uuid.uuid4()}", "_key": str(uuid.uuid4())}
```

## Recommendations for Gemini

1. **Add Dependency Checking**
   - Check for required modules at startup
   - Provide clear installation instructions
   - Support running without full dependencies for testing

2. **Implement Mock Mode**
   - Allow scripts to demonstrate functionality without database
   - Use in-memory storage for testing
   - Provide clear indication when running in mock mode

3. **Improve Error Messages**
   - When imports fail, suggest installation commands
   - Provide links to documentation
   - Explain what functionality is missing

4. **Create Minimal Working Examples**
   - Scripts should work with minimal setup
   - Provide docker-compose for full environment
   - Support gradual feature enablement

## Next Steps

1. Gemini should update the scripts to handle missing dependencies gracefully
2. Add a `--mock` flag to run without database
3. Create a setup script that checks and installs dependencies
4. Update documentation with clear setup instructions

---

**Note**: All mock modules (hybrid_search.py, relationship_extraction.py, memory_agent.py, log_utils.py) work perfectly because they don't have external dependencies. The issue is specifically with the three main scripts that require `aioarango` for database connectivity.