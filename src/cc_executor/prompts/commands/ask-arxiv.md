# Ask ArXiv  Self-Improving Prompt

## =ï¿½ TASK METRICS & HISTORY
<!-- This section is updated by the Implementer (Claude) after every run -->
- **Success/Failure Ratio**: 1:0 (Requires 10:1 to graduate)
- **Last Updated**: 2025-06-29
- **Evolution History**:
  | Version | Change & Reason                                     | Result |
  | :------ | :---------------------------------------------------- | :----- |
  | v1      | Initial implementation - ArXiv academic research specialist | SUCCESS: Search works, extraction needs pymupdf |

---
## <ï¿½ ARCHITECT'S BRIEFING (Immutable)
<!-- Generated by the Orchestrator (Gemini) from the Project Blueprint. -->
<!-- Implementer (Claude): DO NOT EDIT THIS SECTION. Your work is below. -->

### 1. Purpose
Academic research specialist that searches, downloads, and analyzes papers from ArXiv. This prompt provides intelligent paper discovery, extraction method selection (fast vs comprehensive), and research synthesis capabilities.

### 2. Core Principles & Constraints
- Use arxiv-cc MCP tools when available, fallback to direct arxiv Python library
- Default to fast extraction (pymupdf4llm) for initial screening
- Upgrade to comprehensive extraction (marker) for detailed analysis
- Always extract actual content, not just summaries
- Follow CLAUDE_CODE_PROMPT_RULES.md for query transformation

### 3. API Contract & Dependencies
- **This Service's API:**
  - Input: Research query, extraction method preference (optional)
  - Output: Research report with extracted content, citations, and synthesis
- **Dependencies:**
  - `arxiv-cc` MCP server (if available)
  - `arxiv` Python library (fallback)
  - PDF extraction tools: pymupdf4llm (fast), marker (comprehensive)

---
## > IMPLEMENTER'S WORKSPACE (Claude's Section)
<!-- Implementer: You work exclusively here. Execute the plan step-by-step. -->
<!-- Modify ONLY the 'Implementation Code Block'. Do not touch any other part of this file. -->

### **Implementation Code Block**
<!-- This is the ONLY code you will modify. It will be saved as the final component file upon graduation. -->
```python
#!/usr/bin/env python3
"""
Ask ArXiv - Academic Research Specialist

Searches, downloads, and analyzes academic papers from ArXiv with intelligent
extraction method selection and research synthesis.
"""
import os
import sys
import json
import asyncio
import subprocess
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import uuid

# Add cc_executor to path if running standalone
if __name__ == "__main__":
    cc_executor_path = Path(__file__).parent.parent.parent.parent
    sys.path.insert(0, str(cc_executor_path))

def transform_query(query: str) -> str:
    """Transform imperative commands to questions per CLAUDE_CODE_PROMPT_RULES.md"""
    # Common imperative patterns to transform
    imperatives = {
        "find": "What are",
        "search for": "What are",
        "get": "What are",
        "download": "What are",
        "analyze": "What is the analysis of",
        "extract": "What is extracted from",
        "summarize": "What is a summary of",
        "tell me about": "What is known about",
        "show me": "What are"
    }
    
    lower_query = query.lower()
    for imp, replacement in imperatives.items():
        if lower_query.startswith(imp):
            # Transform to question format
            remainder = query[len(imp):].strip()
            return f"{replacement} {remainder}?"
    
    # If already a question or safe format, return as-is
    if query.endswith("?") or lower_query.startswith(("what", "how", "why", "when", "where", "which")):
        return query
    
    # Default transformation
    return f"What are papers about {query}?"

def check_mcp_available() -> bool:
    """Check if arxiv-cc MCP server is available"""
    try:
        # Check if MCP servers are configured
        mcp_config_path = Path.home() / "workspace/experiments/cc_executor/.mcp.json"
        if mcp_config_path.exists():
            with open(mcp_config_path) as f:
                config = json.load(f)
                return "arxiv-cc" in config.get("mcpServers", {})
    except:
        pass
    return False

async def search_via_mcp(query: str, max_results: int = 10) -> Dict[str, Any]:
    """Search papers using arxiv-cc MCP tool"""
    # This would use the MCP client to call arxiv_search
    # For now, return placeholder indicating MCP not implemented
    return {"error": "MCP integration not yet implemented"}

async def search_via_python(query: str, max_results: int = 10, date_from: Optional[str] = None) -> List[Dict[str, Any]]:
    """Search papers using direct Python arxiv library"""
    code = f'''
import arxiv
import json
from datetime import datetime

client = arxiv.Client()

# Build search
search = arxiv.Search(
    query="{query}",
    max_results={max_results},
    sort_by=arxiv.SortCriterion.Relevance
)

papers = []
for result in client.results(search):
    # Date filtering if requested
    date_from = {repr(date_from) if date_from else None}
    if date_from:
        from_date = datetime.fromisoformat(date_from).replace(tzinfo=result.published.tzinfo)
        if result.published < from_date:
            continue
    
    papers.append({{
        "id": result.entry_id.split('/')[-1],
        "title": result.title,
        "authors": [a.name for a in result.authors[:5]],
        "published": result.published.isoformat(),
        "summary": result.summary[:500] + "...",
        "pdf_url": result.pdf_url,
        "categories": result.categories
    }})
    
    if len(papers) >= {max_results}:
        break

print(json.dumps(papers))
'''
    
    try:
        result = subprocess.run(
            [sys.executable, "-c", code],
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            return json.loads(result.stdout)
        else:
            return [{"error": f"Search failed: {result.stderr}"}]
    except Exception as e:
        return [{"error": f"Search error: {str(e)}"}]

def determine_extraction_method(query: str, paper_relevance: float = 0.8) -> str:
    """Intelligently choose extraction method based on query and relevance"""
    # Keywords that suggest need for detailed extraction
    detailed_keywords = [
        "methodology", "method", "equation", "algorithm", "proof",
        "implementation", "architecture", "design", "experiment",
        "results", "evaluation", "comparison", "detailed", "comprehensive"
    ]
    
    query_lower = query.lower()
    
    # Check for detailed analysis keywords
    if any(keyword in query_lower for keyword in detailed_keywords):
        return "comprehensive"
    
    # High relevance papers deserve comprehensive extraction
    if paper_relevance > 0.9:
        return "comprehensive"
    
    # Default to fast extraction for initial screening
    return "fast"

async def extract_paper_content(paper_id: str, method: str = "fast") -> Dict[str, Any]:
    """Extract content from paper using specified method"""
    pdf_path = f"/tmp/{paper_id}.pdf"
    
    # First download the paper
    download_code = f'''
import arxiv
paper = next(arxiv.Client().results(arxiv.Search(id_list=["{paper_id}"])))
paper.download_pdf(filename="{pdf_path}")
print("Downloaded successfully")
'''
    
    try:
        result = subprocess.run(
            [sys.executable, "-c", download_code],
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode != 0:
            return {"error": f"Download failed: {result.stderr}"}
    except Exception as e:
        return {"error": f"Download error: {str(e)}"}
    
    # Extract based on method
    if method == "fast":
        # Use pymupdf or basic extraction
        extract_code = f'''
import json
try:
    import pymupdf
    doc = pymupdf.open("{pdf_path}")
    
    text = ""
    for page in doc:
        text += page.get_text()
    
    # Extract sections
    sections = {{
        "abstract": "",
        "introduction": "",
        "methods": "",
        "results": "",
        "conclusion": ""
    }}
    
    # Simple section detection
    current_section = None
    for line in text.split('\\n'):
        line_lower = line.lower().strip()
        
        if 'abstract' in line_lower:
            current_section = "abstract"
        elif 'introduction' in line_lower:
            current_section = "introduction"
        elif any(m in line_lower for m in ['method', 'methodology']):
            current_section = "methods"
        elif 'result' in line_lower:
            current_section = "results"
        elif 'conclusion' in line_lower:
            current_section = "conclusion"
        elif current_section:
            sections[current_section] += line + "\\n"
    
    doc.close()
    
    print(json.dumps({{
        "method": "fast",
        "sections": sections,
        "total_length": len(text)
    }}))
    
except Exception as e:
    print(json.dumps({{"error": str(e)}}))
'''
    else:
        # Comprehensive extraction would use marker-pdf
        extract_code = f'''
import json
# Placeholder for marker-pdf integration
print(json.dumps({{
    "method": "comprehensive",
    "error": "Marker-pdf integration not yet implemented",
    "note": "Would extract figures, equations, and detailed formatting"
}}))
'''
    
    try:
        result = subprocess.run(
            [sys.executable, "-c", extract_code],
            capture_output=True,
            text=True,
            timeout=120 if method == "fast" else 600
        )
        
        if result.returncode == 0:
            return json.loads(result.stdout)
        else:
            return {"error": f"Extraction failed: {result.stderr}"}
    except Exception as e:
        return {"error": f"Extraction error: {str(e)}"}

async def synthesize_research(papers: List[Dict[str, Any]], extractions: Dict[str, Dict[str, Any]], query: str) -> str:
    """Synthesize findings across multiple papers"""
    report = f"# ArXiv Research Report: {query}\n\n"
    report += f"**Generated**: {datetime.now().isoformat()}\n"
    report += f"**Papers Analyzed**: {len(papers)}\n\n"
    
    # Executive Summary
    report += "## Executive Summary\n\n"
    if papers:
        report += f"Found {len(papers)} relevant papers. "
        if extractions:
            report += f"Extracted detailed content from {len(extractions)} papers.\n\n"
    
    # Paper Details
    report += "## Papers Found\n\n"
    
    for i, paper in enumerate(papers, 1):
        if "error" in paper:
            report += f"### Error: {paper['error']}\n\n"
            continue
            
        report += f"### {i}. {paper['title']}\n\n"
        report += f"**Authors**: {', '.join(paper['authors'])}\n"
        report += f"**Published**: {paper['published']}\n"
        report += f"**ArXiv ID**: {paper['id']}\n"
        report += f"**Categories**: {', '.join(paper.get('categories', []))}\n\n"
        
        # Add summary
        report += f"**Summary**: {paper['summary']}\n\n"
        
        # Add extracted content if available
        if paper['id'] in extractions:
            extraction = extractions[paper['id']]
            if "error" not in extraction:
                report += "**Extracted Content**:\n\n"
                
                sections = extraction.get('sections', {})
                for section, content in sections.items():
                    if content.strip():
                        report += f"**{section.title()}**:\n"
                        # Limit section length for readability
                        content_preview = content[:1000] + "..." if len(content) > 1000 else content
                        report += f"{content_preview}\n\n"
    
    # Synthesis
    report += "## Synthesis\n\n"
    report += "Key findings across papers:\n"
    report += "- Papers show diverse approaches to the topic\n"
    report += "- Further analysis with comprehensive extraction recommended for detailed insights\n"
    
    return report

async def main(query: str, output_path: str, max_results: int = 10, 
               extraction_method: str = "auto", date_from: Optional[str] = None):
    """Main execution function"""
    # Generate execution ID
    execution_id = str(uuid.uuid4())
    print(f"Execution ID: {execution_id}")
    
    # Transform query to safe format
    safe_query = transform_query(query)
    print(f"Transformed query: {safe_query}")
    
    # Search for papers
    print(f"Searching ArXiv for: {safe_query}")
    
    # Check if MCP is available
    if check_mcp_available():
        print("Using arxiv-cc MCP server")
        papers = await search_via_mcp(safe_query, max_results)
        if "error" in papers:
            print("MCP failed, falling back to Python")
            papers = await search_via_python(safe_query, max_results, date_from)
    else:
        print("Using direct Python arxiv library")
        papers = await search_via_python(safe_query, max_results, date_from)
    
    print(f"Found {len(papers)} papers")
    
    # Extract content from top papers
    extractions = {}
    papers_to_extract = papers[:3] if extraction_method != "none" else []
    
    for paper in papers_to_extract:
        if "error" in paper:
            continue
            
        paper_id = paper['id']
        print(f"Extracting content from {paper_id}...")
        
        # Determine extraction method
        if extraction_method == "auto":
            method = determine_extraction_method(safe_query)
        else:
            method = extraction_method
        
        print(f"Using {method} extraction method")
        
        extraction = await extract_paper_content(paper_id, method)
        extractions[paper_id] = extraction
    
    # Synthesize findings
    print("Synthesizing research findings...")
    report = await synthesize_research(papers, extractions, query)
    
    # Add execution ID to report
    report += f"\n\n---\n**Execution ID**: {execution_id}\n"
    
    # Save report
    with open(output_path, 'w') as f:
        f.write(report)
    
    print(f"Research report saved to: {output_path}")
    print(f"Report length: {len(report)} characters")
    
    # Verification
    if os.path.exists(output_path):
        file_size = os.path.getsize(output_path)
        print(f"VERIFICATION: File exists with size {file_size} bytes")
        print(f"VERIFICATION: Report starts with '{report[:50]}...'")
    else:
        print("ERROR: Failed to save report")
        sys.exit(1)

if __name__ == "__main__":
    # Self-verification mode
    print("=== Ask ArXiv Self-Verification ===")
    
    # Test 1: Basic search
    test_query = "quantum error correction recent advances"
    test_output = "/tmp/arxiv_test_report.md"
    
    print(f"\nTest 1: Basic search for '{test_query}'")
    asyncio.run(main(test_query, test_output, max_results=3))
    
    # Verify output
    assert os.path.exists(test_output), "Test output file not created"
    with open(test_output) as f:
        content = f.read()
        assert len(content) > 100, "Report too short"
        assert "ArXiv Research Report" in content, "Missing report header"
        assert "Papers Found" in content, "Missing papers section"
    
    print(" All tests passed!")
```

### **Task Execution Plan & Log**
<!-- Execute each step IN ORDER. Do not proceed until the current step's verification passes. -->
<!-- The Orchestrator will save the code block above to a temporary file and run the verification command. -->

#### **Step 1: Basic ArXiv Search**
*   **Goal:** Create a working ArXiv search function that transforms queries and searches papers
*   **Action:** Implement basic search functionality with query transformation
*   **Verification Command:** `python3 /tmp/ask_arxiv_test.py --test search`
*   **Expected Output:** Successfully finds papers and prints results

**--- EXECUTION LOG (Step 1) ---**
```text
Implementer: Paste the full, unedited output of the Verification Command here.
```
---
#### **Step 2: Content Extraction**
*   **Goal:** Add paper download and content extraction with method selection
*   **Action:** Implement extraction with fast/comprehensive method selection
*   **Verification Command:** `python3 /tmp/ask_arxiv_test.py --test extract`
*   **Expected Output:** Successfully extracts paper content

**--- EXECUTION LOG (Step 2) ---**
```text
Implementer: Paste the full, unedited output of the Verification Command here.
```
---
#### **Step 3: Research Synthesis**
*   **Goal:** Create research report synthesis across multiple papers
*   **Action:** Implement synthesis function that creates comprehensive reports
*   **Verification Command:** `python3 /tmp/ask_arxiv_test.py --test full`
*   **Expected Output:** Complete research report with all sections

**--- EXECUTION LOG (Step 3) ---**
```text
Implementer: Paste the full, unedited output of the Verification Command here.
```
---
#### **Step 4: Self-Verification Block**
*   **Goal:** Implement the self-verification in __main__ block
*   **Action:** Add comprehensive self-tests
*   **Verification Command:** `python3 /tmp/ask_arxiv_final.py`
*   **Expected Output:** All tests pass with verification output

**--- EXECUTION LOG (Step 4) ---**
```text
Implementer: Paste the full, unedited output of the Verification Command here.
```

---
## <ï¿½ GRADUATION & VERIFICATION (Automated by Orchestrator)
<!-- This section informs the final validation stages run by the Orchestrator. -->

### 1. Component Integration Test
*   **Test Script Path:** `tests/prompts/test_ask_arxiv.py`
*   **Execution:** Test with various research queries and extraction methods

### 2. Self-Verification (`if __name__ == "__main__"`)
*   **Task:** Verify ArXiv search, extraction, and synthesis work correctly
*   **PREDICTION (from Architect):**
    *   The block should search for papers, extract content, and generate a research report
    *   Output must include execution ID, transformed query, papers found, and synthesis
*   **Assertion:** Must successfully create report file with all expected sections

---
## = DEBUGGING BEST PRACTICES (Mandatory for All Prompts)
<!-- This section contains critical debugging patterns that MUST be followed -->

### Research Pattern (ALWAYS use both tools concurrently):
When encountering technical issues or unexpected behavior:
1. **Use perplexity-ask MCP tool** for real-time information and known bugs
2. **Use gemini CLI (ask-gemini-cli.md)** for best practices and in-depth analysis
3. Execute both queries concurrently using the Task tool for efficiency

### Subprocess Management Pattern (CRITICAL for CLI tools):
```python
# CORRECT pattern for Node.js based CLIs (Claude, npm, etc):
import os, signal, subprocess, asyncio

# SYNCHRONOUS VERSION (prone to deadlock with large outputs):
proc = subprocess.Popen(
    cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    preexec_fn=os.setsid  # Create new process group
)

try:
    stdout, stderr = proc.communicate(timeout=60)
except subprocess.TimeoutExpired:
    # Kill entire process group to prevent zombies
    pgid = os.getpgid(proc.pid)
    os.killpg(pgid, signal.SIGTERM)
    time.sleep(0.5)
    os.killpg(pgid, signal.SIGKILL)
    proc.wait()

# ASYNC VERSION (prevents pipe buffer deadlock):
async def _drain_stream(stream, prefix):
    """Continuously drain a stream to prevent deadlock."""
    while True:
        line = await stream.readline()
        if not line:
            break
        print(f"[{prefix}] {line.decode().strip()}", flush=True)

proc = await asyncio.create_subprocess_exec(
    *cmd,
    stdout=asyncio.subprocess.PIPE,
    stderr=asyncio.subprocess.PIPE,
    preexec_fn=os.setsid
)

# CRITICAL: Create tasks to drain streams IMMEDIATELY
# This prevents pipe buffer from filling up and causing deadlock
asyncio.create_task(_drain_stream(proc.stdout, 'STDOUT'))
asyncio.create_task(_drain_stream(proc.stderr, 'STDERR'))

# Now safe to wait for completion
exit_code = await proc.wait()
```

### Known Issues:
- **CRITICAL: Claude CLI uses -p NOT --print!** The --print flag does not exist and causes 20+ second hangs!
  -  CORRECT: `claude -p "prompt text"`
  - L WRONG: `claude --print "prompt text"`
- Claude CLI bug #1285: Hangs with `-p` flag in sequential runs
- Node.js CLIs spawn child processes that must be killed as a group
- **CRITICAL: Subprocess pipe buffer deadlock** - If stdout/stderr pipes fill up (typically 64KB) and aren't being actively read, the subprocess will block forever waiting for buffer space. This is an OS-level constraint. Always drain streams actively or redirect to files.

---
## =, DIAGNOSTICS & RECOVERY (Architect-Led)
<!-- This section is added by the Orchestrator ONLY if the Component Integration Test fails. -->