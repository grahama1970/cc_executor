# Research Report: Subprocess Timeouts in Python Asyncio

Generated by Research Collaborator on 2025-06-28

## Executive Summary

The best way to handle subprocess timeouts in Python asyncio is to use `asyncio.create_subprocess_exec()` with `asyncio.timeout()` (Python 3.11+) or `asyncio.wait_for()`, combined with asynchronous stream reading to prevent deadlocks. Key practices include immediately draining stdout/stderr streams, properly killing and waiting for processes on timeout, and avoiding the common pitfall of calling `wait()` without reading from PIPE buffers.

## Detailed Findings

### Query 1: General timeout handling
- Use `asyncio.create_subprocess_exec()` or `asyncio.create_subprocess_shell()` for async subprocess creation
- Apply timeouts with `asyncio.timeout()` context manager (Python 3.11+) or `asyncio.wait_for()`
- On timeout, must call `proc.kill()` followed by `await proc.wait()` to prevent zombie processes
- Don't rely on output frequency for timeout detection - timeout the entire process duration

### Query 2: Handling sporadic output
- Use asynchronous line-by-line reading with `await stream.readline()` 
- This yields control when no data available, preventing blocking
- Process output as it arrives without blocking the event loop
- Check for EOF (empty read) to detect process termination cleanly
- Optionally add timeouts to individual read operations for additional control

### Query 3: Preventing deadlocks
- Primary cause: Subprocess produces more output than pipe buffer can hold (typically 64KB)
- If parent doesn't read from pipes, child blocks waiting for buffer space
- Never use `await proc.wait()` alone with PIPE - this is the classic deadlock pattern
- Either use `communicate()` which handles reading automatically, or implement concurrent stream draining

## Code Examples

### Basic timeout with proper cleanup:
```python
import asyncio

async def run_with_timeout(cmd, timeout):
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    try:
        async with asyncio.timeout(timeout):  # Python 3.11+
            stdout, stderr = await proc.communicate()
    except asyncio.TimeoutError:
        proc.kill()
        await proc.wait()
        raise
    return stdout, stderr
```

### Handling sporadic output without deadlock:
```python
import asyncio

async def _drain_stream(stream, prefix):
    """Continuously drain a stream to prevent buffer overflow"""
    while True:
        line = await stream.readline()
        if not line:
            break
        print(f"[{prefix}] {line.decode().strip()}")

async def run_with_stream_handling(cmd, timeout):
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    
    # Create tasks to drain streams immediately
    drain_tasks = [
        asyncio.create_task(_drain_stream(proc.stdout, 'STDOUT')),
        asyncio.create_task(_drain_stream(proc.stderr, 'STDERR'))
    ]
    
    try:
        # Wait for process with timeout
        async with asyncio.timeout(timeout):
            exit_code = await proc.wait()
    except asyncio.TimeoutError:
        proc.kill()
        await proc.wait()
        # Cancel drain tasks
        for task in drain_tasks:
            task.cancel()
        raise
    
    # Ensure drain tasks complete
    await asyncio.gather(*drain_tasks, return_exceptions=True)
    return exit_code
```

### Process group handling for complex scenarios:
```python
import asyncio
import os
import signal

async def run_with_process_group(cmd, timeout):
    """Run subprocess in its own process group for better control"""
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
        preexec_fn=os.setsid  # Create new process group
    )
    
    # Start draining streams
    asyncio.create_task(_drain_stream(proc.stdout, 'STDOUT'))
    asyncio.create_task(_drain_stream(proc.stderr, 'STDERR'))
    
    try:
        async with asyncio.timeout(timeout):
            exit_code = await proc.wait()
    except asyncio.TimeoutError:
        # Kill entire process group
        os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
        await proc.wait()
        raise
    
    return exit_code
```

## Best Practices

1. **Always drain streams immediately**: Create async tasks to read stdout/stderr as soon as subprocess starts
2. **Use communicate() for simple cases**: When you just need all output at the end
3. **Implement proper cleanup**: On timeout, kill() then wait() to avoid zombies
4. **Create process groups**: For complex subprocesses that may spawn children
5. **Never use wait() with PIPE**: This is the most common source of deadlocks
6. **Set reasonable timeouts**: Based on expected execution time, not output frequency
7. **Handle partial output**: Design your code to work with incomplete output on timeout
8. **Log stream output**: Route to proper logging instead of just printing

## Conclusions

For handling subprocess timeouts in Python asyncio applications with sporadic output, the recommended approach is:

1. Use `asyncio.create_subprocess_exec()` with PIPE for stdout/stderr
2. Immediately create async tasks to drain output streams line-by-line
3. Apply timeout to the overall process using `asyncio.timeout()` or `wait_for()`
4. On timeout, kill the process and await its termination
5. Never call `wait()` without first ensuring streams are being drained

This pattern prevents deadlocks, handles sporadic output gracefully, and ensures proper cleanup of system resources. The key insight is that subprocess pipe buffers are limited (typically 64KB), and filling them without reading causes the subprocess to block indefinitely - a situation that must be actively prevented through concurrent stream reading.

---
*Report generated by Research Collaborator using concurrent AI research tools*